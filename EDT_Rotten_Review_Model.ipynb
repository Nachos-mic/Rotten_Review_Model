{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nachos-mic/Rotten_Review_Model/blob/main/EDT_Rotten_Review_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movie Review Analysis Pipeline\n",
        "IMDB Sentiment Model + Rotten Tomatoes Fake/Biased Review Detection\n"
      ],
      "metadata": {
        "id": "9psosgj3dSEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRENING MODELU 1 - IMDB SENTYMENT"
      ],
      "metadata": {
        "id": "lwZ7_dj3e_Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions -qq\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7_Iwr8_drHL",
        "outputId": "86f1895c-31ac-4d1d-84ff-29e74f6b3953"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.12/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ustawianie środowiska\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import kagglehub\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Instalowanie pakietów NLTK\n",
        "for resource in ['punkt', 'stopwords', 'wordnet', 'punkt_tab', 'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng']:\n",
        "    nltk.download(resource, quiet=True)\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "1ZYYEpyqdUmU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funkcje do czyszczenia datasetu\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_html_tags(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    return soup.get_text()\n",
        "\n",
        "def expand_contractions(text):\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    return text\n",
        "\n",
        "def remove_extra_spaces(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def remove_stopwords_and_lemmatize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [\n",
        "        lemmatizer.lemmatize(token)\n",
        "        for token in tokens\n",
        "        if token not in stop_words and len(token) > 2\n",
        "    ]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def clean_text(text):\n",
        "    # Obsługa NaN, None i pustych wartości\n",
        "    if pd.isna(text) or not isinstance(text, str) or not text.strip():\n",
        "        return ''\n",
        "\n",
        "    text = remove_html_tags(text)\n",
        "    text = expand_contractions(text)\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_special_characters(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "    text = remove_stopwords_and_lemmatize(text)\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB882rInQEEX",
        "outputId": "0c2e2678-82a2-4983-8c49-1794c458c446"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trenowanie modelu do rozpoznawania sentymentu\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Przygotowanie datasetu\n",
        "print(\"=\"*60)\n",
        "print(\"\\nŁadowanie datasetu IMDB...\")\n",
        "\n",
        "\n",
        "dataset_path_imdb = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "csv_path_imdb = os.path.join(dataset_path_imdb, \"IMDB Dataset.csv\")\n",
        "df_imdb = pd.read_csv(csv_path_imdb)\n",
        "\n",
        "print(f\"Załadowano recenzje IMDB reviews\")\n",
        "print(f\"Podział sentymentu:\\n{df_imdb['sentiment'].value_counts()}\")\n",
        "\n",
        "# Przygotowanie datasetu\n",
        "print(\"\\nCzyszczenie datasetu...\")\n",
        "df_imdb['review_cleaned'] = df_imdb['review'].apply(clean_text)\n",
        "df_imdb['sentiment_label'] = df_imdb['sentiment'].map({'positive': 1, 'negative': -1})\n",
        "\n",
        "df_imdb = df_imdb[df_imdb['review_cleaned'].str.strip() != ''].copy()\n",
        "\n",
        "df_imdb_final = df_imdb[['review_cleaned', 'sentiment_label']].copy()\n",
        "df_imdb_final = df_imdb_final.rename(columns={'review_cleaned': 'text', 'sentiment_label': 'label'})\n",
        "df_imdb_final['source'] = 'IMDB'\n",
        "\n",
        "print(f\"\\nPrzygotowano dane: {df_imdb_final.shape}\")\n",
        "print(f\"Podział etykiet:\")\n",
        "print(df_imdb_final['label'].value_counts())\n",
        "print(f\"Pozytywne (1): {(df_imdb_final['label'] == 1).sum()} ({(df_imdb_final['label'] == 1).sum()/len(df_imdb_final)*100:.1f}%)\")\n",
        "print(f\"Negatywne (-1): {(df_imdb_final['label'] == -1).sum()} ({(df_imdb_final['label'] == -1).sum()/len(df_imdb_final)*100:.1f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1XpC2EVPqO-",
        "outputId": "9f8e3020-21ca-458d-da73-76cbc1a9fc3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Ładowanie datasetu IMDB\n",
            "============================================================\n",
            "Using Colab cache for faster access to the 'imdb-dataset-of-50k-movie-reviews' dataset.\n",
            "Załadowano recenzje IMDB reviews\n",
            "Podział sentymentu:\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Czyszczenie datasetu\n",
            "\n",
            "Przygotowano dane: (50000, 3)\n",
            "Podział etykiet:\n",
            "label\n",
            " 1    25000\n",
            "-1    25000\n",
            "Name: count, dtype: int64\n",
            "Pozytywne (1): 25000 (50.0%)\n",
            "Negatywne (-1): 25000 (50.0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ładowanie datasetu Rotten Tomatoes\n",
        "print(\"Ładowanie datasetu Clapper Rotten Tomatoes\")\n",
        "\n",
        "dataset_path_clapper = kagglehub.dataset_download(\n",
        "    \"andrezaza/clapper-massive-rotten-tomatoes-movies-and-reviews\"\n",
        ")\n",
        "\n",
        "csv_files = glob.glob(os.path.join(dataset_path_clapper, \"*.csv\"))\n",
        "print(f\"Znaleziono pliki: {[os.path.basename(f) for f in csv_files]}\")\n",
        "\n",
        "reviews_file = None\n",
        "for f in csv_files:\n",
        "    fname = os.path.basename(f).lower()\n",
        "    if 'review' in fname and 'movie' not in fname:\n",
        "        reviews_file = f\n",
        "        break\n",
        "    elif 'movie_review' in fname:\n",
        "        reviews_file = f\n",
        "        break\n",
        "\n",
        "if not reviews_file and csv_files:\n",
        "    for f in csv_files:\n",
        "        if 'review' in os.path.basename(f).lower():\n",
        "            reviews_file = f\n",
        "            break\n",
        "    if not reviews_file:\n",
        "        reviews_file = csv_files[0]\n",
        "\n",
        "df_clapper = pd.read_csv(reviews_file)\n",
        "\n",
        "# Filtracja top-krytyków\n",
        "top_critic_col = None\n",
        "for col in df_clapper.columns:\n",
        "    col_lower = col.lower().replace('_', '').replace(' ', '')\n",
        "    if 'topcritic' in col_lower or 'istopdcritic' in col_lower:\n",
        "        top_critic_col = col\n",
        "        break\n",
        "\n",
        "if top_critic_col:\n",
        "    print(f\"\\nFiltracja top-krytyków (przed: {len(df_clapper)})\")\n",
        "    df_clapper = df_clapper[\n",
        "        (df_clapper[top_critic_col] == True) |\n",
        "        (df_clapper[top_critic_col] == 1) |\n",
        "        (df_clapper[top_critic_col] == 'True') |\n",
        "        (df_clapper[top_critic_col] == 'true')\n",
        "    ].copy()\n",
        "    print(f\"   Po filtracji: {len(df_clapper)}\")\n",
        "else:\n",
        "    print(\"\\nKolumna top-krytyków nie znaleziona - używam wszystkich recenzji\")\n",
        "\n",
        "# Detekcja kolumn\n",
        "text_col = None\n",
        "label_col = None\n",
        "score_sentiment_col = None\n",
        "\n",
        "for col in df_clapper.columns:\n",
        "    col_lower = col.lower().replace('_', '').replace(' ', '')\n",
        "\n",
        "    if any(x in col_lower for x in ['reviewtext', 'text', 'content']):\n",
        "        if text_col is None:\n",
        "            text_col = col\n",
        "\n",
        "    if any(x in col_lower for x in ['reviewstate', 'state', 'type']):\n",
        "        if 'score' not in col_lower:\n",
        "            label_col = col\n",
        "\n",
        "    if 'scoresentiment' in col_lower or ('score' in col_lower and 'sentiment' in col_lower):\n",
        "        score_sentiment_col = col\n",
        "\n",
        "if score_sentiment_col:\n",
        "    df_clapper = df_clapper.drop(columns=[score_sentiment_col])\n",
        "\n",
        "if text_col is None or label_col is None:\n",
        "    raise ValueError(\"Nie znaleziono wymaganych kolumn text/label\")\n",
        "\n",
        "# Czyszczenie tekstu\n",
        "print(f\"\\nCzyszczenie tekstu (przed: {len(df_clapper)})\")\n",
        "df_clapper = df_clapper[df_clapper[text_col].notna()].copy()\n",
        "df_clapper['text_cleaned'] = df_clapper[text_col].apply(clean_text)\n",
        "df_clapper = df_clapper[df_clapper['text_cleaned'].str.strip() != ''].copy()\n",
        "print(f\"   Po czyszczeniu: {len(df_clapper)}\")\n",
        "\n",
        "# Mapowanie etykiet\n",
        "def map_rt_sentiment(val):\n",
        "    if pd.isna(val):\n",
        "        return None\n",
        "    val_str = str(val).lower().strip()\n",
        "    if val_str in ['fresh', 'positive', '1', 'pos']:\n",
        "        return 1\n",
        "    elif val_str in ['rotten', 'negative', '0', 'neg']:\n",
        "        return -1\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "df_clapper['sentiment_label'] = df_clapper[label_col].apply(map_rt_sentiment)\n",
        "df_clapper = df_clapper.dropna(subset=['sentiment_label']).copy()\n",
        "\n",
        "# Finalne przygotowanie\n",
        "df_clapper_final = df_clapper[['text_cleaned', 'sentiment_label']].copy()\n",
        "df_clapper_final = df_clapper_final.rename(columns={'text_cleaned': 'text', 'sentiment_label': 'label'})\n",
        "df_clapper_final['label'] = df_clapper_final['label'].astype(int)\n",
        "df_clapper_final['source'] = 'Clapper_RT'\n",
        "\n",
        "print(f\"\\nClapper RT przygotowany: {df_clapper_final.shape}\")\n",
        "print(f\"   Podział etykiet: {df_clapper_final['label'].value_counts().to_dict()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x350jnY9PtOW",
        "outputId": "4ea24b4d-70e0-480c-f6ac-b29974bd5849"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ładowanie datasetu Clapper Rotten Tomatoes\n",
            "Using Colab cache for faster access to the 'clapper-massive-rotten-tomatoes-movies-and-reviews' dataset.\n",
            "Znaleziono pliki: ['rotten_tomatoes_movies.csv', 'rotten_tomatoes_movie_reviews.csv']\n",
            "\n",
            "Filtracja top-krytyków (przed: 1444963)\n",
            "   Po filtracji: 436807\n",
            "\n",
            "Czyszczenie tekstu (przed: 436807)\n",
            "   Po czyszczeniu: 426268\n",
            "\n",
            "Clapper RT przygotowany: (426268, 3)\n",
            "   Podział etykiet: {1: 269728, -1: 156540}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Łączenie datasetu IMDB i Rotten\n",
        "print(\"Łączenie datasetu\")\n",
        "\n",
        "# Połącz oba datasety\n",
        "df_combined = pd.concat([df_imdb_final, df_clapper_final], ignore_index=True)\n",
        "\n",
        "# Usuń duplikaty tekstowe\n",
        "print(f\"\\nUsuwanie duplikatów...\")\n",
        "original_size = len(df_combined)\n",
        "df_combined = df_combined.drop_duplicates(subset=['text'], keep='first')\n",
        "duplicates_removed = original_size - len(df_combined)\n",
        "print(f\"Usunięto {duplicates_removed} duplikatów ({duplicates_removed/original_size*100:.1f}%)\")\n",
        "\n",
        "# Losowe przemieszanie\n",
        "df_combined = df_combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nPołączony dataset: {df_combined.shape}\")\n",
        "print(\"\\nPodział datasetu:\")\n",
        "print(f\"  IMDB: {(df_combined['source'] == 'IMDB').sum()}\")\n",
        "print(f\"  Clapper RT: {(df_combined['source'] == 'Clapper_RT').sum()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXo8fS8ePwP3",
        "outputId": "06a05d97-1295-4ffe-ea92-7c9ead165bcd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Łączenie datasetu\n",
            "\n",
            "Usuwanie duplikatów...\n",
            "Usunięto 5391 duplikatów (1.1%)\n",
            "\n",
            "Połączony dataset: (470877, 3)\n",
            "\n",
            "Podział datasetu:\n",
            "  IMDB: 49574\n",
            "  Clapper RT: 421303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trening datasetu\n",
        "print(\"Trening modelu\")\n",
        "\n",
        "print(\"\\nWektoryzacja tekstu...\")\n",
        "vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2), min_df=2)\n",
        "X = vectorizer.fit_transform(df_combined['text'])\n",
        "y = df_combined['label']\n",
        "\n",
        "# Podział na zbiory train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nStatystyki podziału na zbiory:\")\n",
        "print(f\"   Treningowy: {X_train.shape[0]}\")\n",
        "print(f\"   Testowy: {X_test.shape[0]}\")\n",
        "print(f\"   Podział etykiet: {pd.Series(y_train).value_counts().to_dict()}\")\n",
        "\n",
        "# Trenowanie\n",
        "model = LogisticRegression(max_iter=1000, random_state=42, C=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Ewaluacja\n",
        "print(\"\\nEwaluacja modelu...\")\n",
        "y_pred_train = model.predict(X_train)\n",
        "y_pred_test = model.predict(X_test)\n",
        "\n",
        "train_acc = accuracy_score(y_train, y_pred_train)\n",
        "test_acc = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"   Train Accuracy: {train_acc:.4f}\")\n",
        "print(f\"   Test Accuracy:  {test_acc:.4f}\")\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_pred_test,\n",
        "                          target_names=['Negative (-1)', 'Positive (1)']))\n",
        "\n",
        "#Macierz pomyłek\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "id": "DAUceU3-P0Ge",
        "outputId": "97b54df5-f11d-479c-9b7c-0f7f42e6bac1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trening modelu\n",
            "\n",
            "Wektoryzacja tekstu...\n",
            "\n",
            "Statystyki podziału na zbiory:\n",
            "   Treningowy: 376701\n",
            "   Testowy: 94176\n",
            "   Podział etykiet: {1: 233106, -1: 143595}\n",
            "\n",
            "Ewaluacja modelu...\n",
            "   Train Accuracy: 0.8174\n",
            "   Test Accuracy:  0.8016\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.77      0.68      0.72     35899\n",
            " Positive (1)       0.82      0.87      0.85     58277\n",
            "\n",
            "     accuracy                           0.80     94176\n",
            "    macro avg       0.79      0.78      0.78     94176\n",
            " weighted avg       0.80      0.80      0.80     94176\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 2 - BIAS ROTTEN TOMATOES"
      ],
      "metadata": {
        "id": "5j76V25ffLOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA & PREPROCESSING"
      ],
      "metadata": {
        "id": "kEmpJV6jfTL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PART 2: ROTTEN TOMATOES DATA LOADING\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PART 2: LOADING ROTTEN TOMATOES DATA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "dataset_path_rt = kagglehub.dataset_download(\n",
        "    \"stefanoleone992/rotten-tomatoes-movies-and-critic-reviews-dataset\"\n",
        ")\n",
        "\n",
        "movies_csv = os.path.join(dataset_path_rt, \"rotten_tomatoes_movies.csv\")\n",
        "reviews_csv = os.path.join(dataset_path_rt, \"rotten_tomatoes_critic_reviews.csv\")\n",
        "\n",
        "movies_df = pd.read_csv(movies_csv)\n",
        "reviews_df = pd.read_csv(reviews_csv)\n",
        "\n",
        "print(f\"\\n📊 Movies: {movies_df.shape}\")\n",
        "print(f\"📊 Reviews: {reviews_df.shape}\")"
      ],
      "metadata": {
        "id": "5c8D4zq5e9wB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analiza rozkładu sentiment/review_type\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "if 'tomatometer_status' in movies_df.columns:\n",
        "    movies_df['tomatometer_status'].value_counts().plot(kind='bar', ax=axes[0], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
        "    axes[0].set_title('Movies: Tomatometer Status Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Status')\n",
        "    axes[0].set_ylabel('Count')\n",
        "\n",
        "if 'review_type' in reviews_df.columns:\n",
        "    reviews_df['review_type'].value_counts().plot(kind='bar', ax=axes[1], color=['#00EA6E', '#FF3B3F'])\n",
        "    axes[1].set_title('Reviews: Review Type Distribution', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Review Type')\n",
        "    axes[1].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rhKVQvMSfssp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df['review_score'].dropna().unique()\n"
      ],
      "metadata": {
        "id": "VtRIYuR0fuqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EDA 1: Initial Data Exploration\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EDA 1: INITIAL DATA QUALITY CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n🔍 Reviews Dataset:\")\n",
        "print(f\"  Total reviews: {len(reviews_df)}\")\n",
        "print(f\"  Missing values:\\n{reviews_df.isnull().sum()[reviews_df.isnull().sum() > 0]}\")\n",
        "print(f\"\\n  Top Critic Distribution:\")\n",
        "print(reviews_df['top_critic'].value_counts(dropna=False))\n",
        "\n",
        "print(\"\\n🔍 Movies Dataset:\")\n",
        "print(f\"  Total movies: {len(movies_df)}\")\n",
        "# Convert 'original_release_date' to datetime, coercing errors\n",
        "movies_df['original_release_date'] = pd.to_datetime(movies_df['original_release_date'], errors='coerce')\n",
        "print(f\"  Date range: {movies_df['original_release_date'].min()} to {movies_df['original_release_date'].max()}\")"
      ],
      "metadata": {
        "id": "sbXmplkRfbSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREPROCESSING 1: Filter Top Critics Only\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING 1: FILTERING TOP CRITICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "original_count = len(reviews_df)\n",
        "reviews_df = reviews_df[reviews_df['top_critic'] == True].copy()\n",
        "print(f\"✂️ Filtered from {original_count} to {len(reviews_df)} reviews (top critics only)\")\n",
        "print(f\"   Kept {len(reviews_df)/original_count*100:.1f}% of data\")"
      ],
      "metadata": {
        "id": "S-QZspBnfymz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df['review_score'].dropna().unique()\n"
      ],
      "metadata": {
        "id": "q5qCJlszf49Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tu trzeba zrobić rzutowanie na liczby całkowite od 0/1 do 100"
      ],
      "metadata": {
        "id": "IgmNNpWPc0RN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREPROCESSING 2: Merge Datasets\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING 2: MERGING REVIEWS WITH MOVIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Select relevant movie columns\n",
        "movie_cols = ['rotten_tomatoes_link', 'original_release_date', 'tomatometer_rating',\n",
        "              'tomatometer_status', 'audience_rating', 'audience_status', 'movie_title']\n",
        "movies_subset = movies_df[movie_cols].copy()\n",
        "\n",
        "# Merge\n",
        "df = reviews_df.merge(movies_subset, on='rotten_tomatoes_link', how='left')\n",
        "print(f\"✅ Merged dataset: {df.shape}\")\n",
        "print(f\"   Merge success rate: {(1 - df['original_release_date'].isna().sum()/len(df))*100:.1f}%\")"
      ],
      "metadata": {
        "id": "FGdFNjPyf-V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review_date'] = pd.to_datetime(df['review_date'])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.hist(\n",
        "    df['review_date'],\n",
        "    bins=50,\n",
        "    density=True\n",
        ")\n",
        "\n",
        "plt.xlabel(\"Review Date\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Distribution of Review Publication Dates\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zwGO9IrDRGyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EDA 2: Text Quality Analysis\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EDA 2: TEXT CONTENT ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Sample reviews to check for artifacts\n",
        "sample_reviews = df['review_content'].dropna().head(10)\n",
        "print(\"\\n📝 Sample review (first 200 chars):\")\n",
        "print(sample_reviews.iloc[0][:200])\n",
        "\n",
        "# Check for HTML tags\n",
        "html_pattern = re.compile('<.*?>')\n",
        "has_html = df['review_content'].str.contains(html_pattern, na=False).sum()\n",
        "print(f\"\\n🔍 Reviews with HTML tags: {has_html}\")\n",
        "\n",
        "# Check for URLs\n",
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "has_urls = df['review_content'].str.contains(url_pattern, na=False).sum()\n",
        "print(f\"🔍 Reviews with URLs: {has_urls}\")\n",
        "\n",
        "# Text length distribution\n",
        "df['review_length_raw'] = df['review_content'].str.len()\n",
        "print(f\"\\n📏 Review length stats:\")\n",
        "print(df['review_length_raw'].describe())"
      ],
      "metadata": {
        "id": "XNiYPKGqf_XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREPROCESSING 3: Text Cleaning\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING 3: CLEANING TEXT CONTENT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "CHAT_WORDS = {\n",
        "    \"AFAIK\": \"As Far As I Know\",\n",
        "    \"AFK\": \"Away From Keyboard\",\n",
        "    \"ASAP\": \"As Soon As Possible\",\n",
        "    \"ATK\": \"At The Keyboard\",\n",
        "    \"ATM\": \"At The Moment\",\n",
        "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
        "    \"BAK\": \"Back At Keyboard\",\n",
        "    \"BBL\": \"Be Back Later\",\n",
        "    \"BBS\": \"Be Back Soon\",\n",
        "    \"BFN\": \"Bye For Now\",\n",
        "    \"B4N\": \"Bye For Now\",\n",
        "    \"BRB\": \"Be Right Back\",\n",
        "    \"BRT\": \"Be Right There\",\n",
        "    \"BTW\": \"By The Way\",\n",
        "    \"B4\": \"Before\",\n",
        "    \"B4N\": \"Bye For Now\",\n",
        "    \"CU\": \"See You\",\n",
        "    \"CUL8R\": \"See You Later\",\n",
        "    \"CYA\": \"See You\",\n",
        "    \"FAQ\": \"Frequently Asked Questions\",\n",
        "    \"FC\": \"Fingers Crossed\",\n",
        "    \"FWIW\": \"For What It's Worth\",\n",
        "    \"FYI\": \"For Your Information\",\n",
        "    \"GAL\": \"Get A Life\",\n",
        "    \"GG\": \"Good Game\",\n",
        "    \"GN\": \"Good Night\",\n",
        "    \"GMTA\": \"Great Minds Think Alike\",\n",
        "    \"GR8\": \"Great!\",\n",
        "    \"G9\": \"Genius\",\n",
        "    \"IC\": \"I See\",\n",
        "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
        "    \"ILU\": \"ILU: I Love You\",\n",
        "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
        "    \"IMO\": \"In My Opinion\",\n",
        "    \"IOW\": \"In Other Words\",\n",
        "    \"IRL\": \"In Real Life\",\n",
        "    \"KISS\": \"Keep It Simple, Stupid\",\n",
        "    \"LDR\": \"Long Distance Relationship\",\n",
        "    \"LMAO\": \"Laugh My A.. Off\",\n",
        "    \"LOL\": \"Laughing Out Loud\",\n",
        "    \"LTNS\": \"Long Time No See\",\n",
        "    \"L8R\": \"Later\",\n",
        "    \"MTE\": \"My Thoughts Exactly\",\n",
        "    \"M8\": \"Mate\",\n",
        "    \"NRN\": \"No Reply Necessary\",\n",
        "    \"OIC\": \"Oh I See\",\n",
        "    \"PITA\": \"Pain In The A..\",\n",
        "    \"PRT\": \"Party\",\n",
        "    \"PRW\": \"Parents Are Watching\",\n",
        "    \"QPSA?\": \"Que Pasa?\",\n",
        "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
        "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
        "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
        "    \"SK8\": \"Skate\",\n",
        "    \"STATS\": \"Your sex and age\",\n",
        "    \"ASL\": \"Age, Sex, Location\",\n",
        "    \"THX\": \"Thank You\",\n",
        "    \"TTFN\": \"Ta-Ta For Now!\",\n",
        "    \"TTYL\": \"Talk To You Later\",\n",
        "    \"U\": \"You\",\n",
        "    \"U2\": \"You Too\",\n",
        "    \"U4E\": \"Yours For Ever\",\n",
        "    \"WB\": \"Welcome Back\",\n",
        "    \"WTF\": \"What The F...\",\n",
        "    \"WTG\": \"Way To Go!\",\n",
        "    \"WUF\": \"Where Are You From?\",\n",
        "    \"W8\": \"Wait...\",\n",
        "    \"7K\": \"Sick:-D Laugher\",\n",
        "    \"TFW\": \"That feeling when\",\n",
        "    \"MFW\": \"My face when\",\n",
        "    \"MRW\": \"My reaction when\",\n",
        "    \"IFYP\": \"I feel your pain\",\n",
        "    \"TNTL\": \"Trying not to laugh\",\n",
        "    \"JK\": \"Just kidding\",\n",
        "    \"IDC\": \"I don't care\",\n",
        "    \"ILY\": \"I love you\",\n",
        "    \"IMU\": \"I miss you\",\n",
        "    \"ADIH\": \"Another day in hell\",\n",
        "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
        "    \"WYWH\": \"Wish you were here\",\n",
        "    \"TIME\": \"Tears in my eyes\",\n",
        "    \"BAE\": \"Before anyone else\",\n",
        "    \"FIMH\": \"Forever in my heart\",\n",
        "    \"BSAAW\": \"Big smile and a wink\",\n",
        "    \"BWL\": \"Bursting with laughter\",\n",
        "    \"BFF\": \"Best friends forever\",\n",
        "    \"CSL\": \"Can't stop laughing\"\n",
        "}\n",
        "\n",
        "def remove_urls(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    return re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "def remove_emoji(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emotikony\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbole i piktogramy\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport i symbole map\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flagi (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chińskie znaki/symbole\n",
        "        u\"\\U00002702-\\U000027B0\"  # dingbats\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001F926-\\U0001F937\"  # dodatkowe twarze i gesty\n",
        "        u\"\\U00010000-\\U0010ffff\"  # dodatkowe symbole\n",
        "        u\"\\U0001F900-\\U0001F9FF\"  # symbole emotikony (2017+)\n",
        "        u\"\\U0001FA00-\\U0001FA6F\"  # symbole rozszerzone-A\n",
        "        u\"\\U0001FA70-\\U0001FAFF\"  # symbole rozszerzone-B\n",
        "        u\"\\u2640-\\u2642\"           # symbole płci\n",
        "        u\"\\u2600-\\u2B55\"           # różne symbole\n",
        "        u\"\\u200d\"                  # zero width joiner\n",
        "        u\"\\ufe0f\"                  # variation selector\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\u3030\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub('', text)\n",
        "\n",
        "\n",
        "def expand_chat_words(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    words = text.split()\n",
        "    expanded = [CHAT_WORDS.get(word.upper(), word) for word in words]\n",
        "    return ' '.join(expanded)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    if not isinstance(text, str):\n",
        "        return ''\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def clean_text_rt(text):\n",
        "    \"\"\"Rotten Tomatoes cleaning pipeline\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return ''\n",
        "\n",
        "    text = remove_html_tags(text)\n",
        "    text = remove_urls(text)\n",
        "    text = remove_emoji(text)\n",
        "    text = expand_chat_words(text)\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    text = remove_extra_spaces(text)\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"🧹 Cleaning review content...\")\n",
        "df['text_cleaned'] = df['review_content'].apply(clean_text_rt)\n",
        "\n",
        "empty_after_cleaning = (df['text_cleaned'].str.strip() == '').sum()\n",
        "print(f\"✅ Cleaned {len(df)} reviews\")\n",
        "print(f\"   Empty after cleaning: {empty_after_cleaning}\")"
      ],
      "metadata": {
        "id": "wvwdMl-af_mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EDA 3: Post-Cleaning Analysis\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EDA 3: POST-CLEANING QUALITY CHECK\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "df['text_length_cleaned'] = df['text_cleaned'].str.len()\n",
        "print(\"📏 Cleaned text length distribution:\")\n",
        "print(df['text_length_cleaned'].describe())\n",
        "\n",
        "print(\"\\n📝 Sample cleaned review:\")\n",
        "print(df['text_cleaned'].iloc[0][:200])"
      ],
      "metadata": {
        "id": "NKL6WGnigYeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREPROCESSING 4: Tokenization & Lemmatization\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING 4: TOKENIZATION & LEMMATIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'V': wordnet.VERB,\n",
        "        'N': wordnet.NOUN,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    return tag_dict.get(treebank_tag[0], wordnet.NOUN)\n",
        "\n",
        "def tokenize_and_lemmatize(text, remove_stopwords=True, min_word_length=2):\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word.isalpha() and len(word) >= min_word_length]\n",
        "\n",
        "    if remove_stopwords:\n",
        "        stop_words_set = set(stopwords.words('english'))\n",
        "        custom_stopwords = {'film', 'movie', 'movies', 'films', 'scene', 'scenes'}\n",
        "        stop_words_set.update(custom_stopwords)\n",
        "        tokens = [word for word in tokens if word not in stop_words_set]\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "                  for word, pos in pos_tags]\n",
        "\n",
        "    return lemmatized\n",
        "\n",
        "print(\"🔤 Tokenizing and lemmatizing...\")\n",
        "df['tokens'] = df['text_cleaned'].apply(tokenize_and_lemmatize)\n",
        "df['token_count'] = df['tokens'].apply(len)\n",
        "df['tokens_text'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "print(f\"✅ Tokenization complete\")\n",
        "print(f\"   Avg tokens per review: {df['token_count'].mean():.1f}\")\n",
        "print(f\"   Median tokens: {df['token_count'].median():.0f}\")"
      ],
      "metadata": {
        "id": "3MHA-JjPgbWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREPROCESSING 5: Date Processing\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING 5: DATE FEATURE EXTRACTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "df['review_date'] = pd.to_datetime(df['review_date'], errors='coerce')\n",
        "df['original_release_date'] = pd.to_datetime(df['original_release_date'], errors='coerce')\n",
        "\n",
        "df['review_year'] = df['review_date'].dt.year\n",
        "df['review_month'] = df['review_date'].dt.month\n",
        "df['release_year'] = df['original_release_date'].dt.year\n",
        "\n",
        "df['days_since_release'] = (df['review_date'] - df['original_release_date']).dt.days\n",
        "\n",
        "print(f\"✅ Date features extracted\")\n",
        "print(f\"   Reviews with valid dates: {df['days_since_release'].notna().sum()}\")\n",
        "print(f\"   Date range: {df['review_date'].min()} to {df['review_date'].max()}\")"
      ],
      "metadata": {
        "id": "AWxaZzHHgdq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EDA 4: Temporal Patterns\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EDA 4: REVIEW TIMING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "early_reviews = (df['days_since_release'] < 7).sum()\n",
        "print(f\"🚨 Early reviews (< 7 days): {early_reviews} ({early_reviews/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nDays since release distribution:\")\n",
        "print(df['days_since_release'].describe())"
      ],
      "metadata": {
        "id": "UtO2MNOggjt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EDA 4: Temporal Patterns\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EDA 4: REVIEW TIMING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "early_reviews = (df['days_since_release'] < 7).sum()\n",
        "print(f\"🚨 Early reviews (< 7 days): {early_reviews} ({early_reviews/len(df)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\nDays since release distribution:\")\n",
        "print(df['days_since_release'].describe())"
      ],
      "metadata": {
        "id": "lUYX6aKOI1Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "df['days_since_release'].plot.hist(\n",
        "    bins=60,\n",
        "    density=True\n",
        ")\n",
        "\n",
        "plt.axvline(0)\n",
        "plt.xlabel(\"Days Since Release\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Review Timing Distribution (Normalized)\")\n",
        "\n",
        "plt.xlim(-60, 180)  # zakres czytelny, bez długiego ogona\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DqqkOKKMPbaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "df[\n",
        "    (df['days_since_release'] >= -400) &\n",
        "    (df['days_since_release'] <= 14)\n",
        "]['days_since_release'].plot.hist(\n",
        "    bins=28,\n",
        "    density=True\n",
        ")\n",
        "\n",
        "plt.axvline(0)\n",
        "plt.xlabel(\"Days Since Release\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.title(\"Review Distribution Around Release (±14 Days)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ziXqhJPhPFXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.sort_values('days_since_release').head(5)[\n",
        "    ['movie_title', 'days_since_release', 'review_date', 'original_release_date']\n",
        "]\n"
      ],
      "metadata": {
        "id": "-jkTtVgiP-_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upewnij się, że daty są datetime\n",
        "df['review_date'] = pd.to_datetime(df['review_date'])\n",
        "df['original_release_date'] = pd.to_datetime(df['original_release_date'])\n",
        "\n",
        "# znajdź najnowszy film (max release_date)\n",
        "latest_release_date = df['original_release_date'].max()\n",
        "\n",
        "# wszystkie recenzje tego filmu\n",
        "latest_movie_reviews = df[df['original_release_date'] == latest_release_date]\n",
        "\n",
        "# najstarsza recenzja dla tego filmu\n",
        "oldest_review_latest_movie = latest_movie_reviews.sort_values('review_date').head(1)\n",
        "\n",
        "oldest_review_latest_movie[\n",
        "    ['movie_title', 'original_release_date', 'review_date', 'days_since_release']\n",
        "]\n"
      ],
      "metadata": {
        "id": "AnnO5wvfR5Aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREPROCESSING 6: Feature Engineering\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING 6: FEATURE ENGINEERING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Categorical encoding\n",
        "df['review_type_encoded'] = df['review_type'].map({'Fresh': 1, 'Rotten': 0})\n",
        "\n",
        "# Text features\n",
        "df['word_count'] = df['text_cleaned'].str.split().str.len()\n",
        "df['sentence_count'] = df['text_cleaned'].str.count(r'[.!?]') + 1\n",
        "df['avg_word_length'] = df['text_length_cleaned'] / df['word_count'].replace(0, 1)\n",
        "df['unique_word_count'] = df['tokens'].apply(lambda x: len(set(x)))\n",
        "df['lexical_diversity'] = df['unique_word_count'] / df['token_count'].replace(0, 1)\n",
        "\n",
        "# Behavioral features\n",
        "df['is_early_review'] = df['days_since_release'] < 7\n",
        "\n",
        "print(\"✅ Feature engineering complete\")\n",
        "print(f\"   Total features created: text (7) + temporal (5) + categorical (1)\")"
      ],
      "metadata": {
        "id": "9XaDGKKKg0Vw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREPROCESSING 7: Apply IMDB Sentiment\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREPROCESSING 7: APPLYING IMDB SENTIMENT MODEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"🎯 Predicting sentiment for Rotten Tomatoes reviews...\")\n",
        "df['predicted_sentiment'] = df['text_cleaned'].apply(predict_sentiment)\n",
        "\n",
        "sentiment_dist = df['predicted_sentiment'].value_counts()\n",
        "print(f\"\\n✅ Sentiment predictions:\")\n",
        "print(f\"   Positive (1): {sentiment_dist.get(1, 0)} ({sentiment_dist.get(1, 0)/len(df)*100:.1f}%)\")\n",
        "print(f\"   Negative (-1): {sentiment_dist.get(-1, 0)} ({sentiment_dist.get(-1, 0)/len(df)*100:.1f}%)\")"
      ],
      "metadata": {
        "id": "m-kE0F4sg3qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# EDA 5: Sentiment vs Review Type\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EDA 5: SENTIMENT MODEL VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "crosstab = pd.crosstab(\n",
        "    df['review_type'],\n",
        "    df['predicted_sentiment'],\n",
        "    normalize='index'\n",
        ") * 100\n",
        "\n",
        "print(\"\\n📊 Predicted Sentiment by Review Type (%):\")\n",
        "print(crosstab.round(1))\n",
        "\n",
        "# Agreement rate\n",
        "df['sentiment_matches'] = (\n",
        "    ((df['review_type'] == 'Fresh') & (df['predicted_sentiment'] == 1)) |\n",
        "    ((df['review_type'] == 'Rotten') & (df['predicted_sentiment'] == -1))\n",
        ")\n",
        "agreement = df['sentiment_matches'].mean() * 100\n",
        "print(f\"\\n✅ Agreement rate: {agreement:.1f}%\")"
      ],
      "metadata": {
        "id": "xgoWD3Eqg_MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "wywalmy z chmury części wspólne może"
      ],
      "metadata": {
        "id": "uTguucBPFl7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# VISUALIZATION: Word Clouds\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING WORD CLOUDS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def generate_wordcloud(tokens_series, title):\n",
        "    all_tokens = [token for tokens in tokens_series for token in tokens]\n",
        "    if not all_tokens:\n",
        "        return\n",
        "\n",
        "    text = ' '.join(all_tokens)\n",
        "    wordcloud = WordCloud(\n",
        "        width=1200, height=600,\n",
        "        background_color='white',\n",
        "        max_words=100,\n",
        "        colormap='viridis'\n",
        "    ).generate(text)\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title, fontsize=18, fontweight='bold')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "fresh_reviews = df[df['review_type'] == 'Fresh']\n",
        "rotten_reviews = df[df['review_type'] == 'Rotten']\n",
        "\n",
        "generate_wordcloud(fresh_reviews['tokens'], 'Fresh Reviews - Most Common Words')\n",
        "generate_wordcloud(rotten_reviews['tokens'], 'Rotten Reviews - Most Common Words')"
      ],
      "metadata": {
        "id": "RgOz0DQhhBs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mismatched_df = df[~df['sentiment_matches']]\n",
        "\n",
        "print(f\"❌ Liczba niezgodnych recenzji: {len(mismatched_df)} ({len(mismatched_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "mismatched_df[['review_type', 'predicted_sentiment', 'text_cleaned']].sample(10)\n"
      ],
      "metadata": {
        "id": "R0sqFAg-jibV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LISTA ZMIAN PRZED NAUCZANIEM REGRESJI:\n",
        "\n",
        "- worldcloud podzielić na fresh i rotten bo z wspólnej chmury ciężko coś wyczytać sensownego\n",
        "\n",
        "- potencjalnie usunąć absurdalne przypadki pokroju \"recenzja napisana w roku 1800\", bo mogą potencjalnie psuć działanie modelu nr.3"
      ],
      "metadata": {
        "id": "KS7kgiz8kxRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Na razie tutaj bazowa regresja na podstawie tego co wcześniej ustalono"
      ],
      "metadata": {
        "id": "Ma5TzbEeNPxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PARSOWANIE REVIEW_SCORE: 0-100 → 1-10\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PARSING REVIEW_SCORE TO 1-10 SCALE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Mapowanie ocen literowych na 0-100 (pośredni krok)\n",
        "GRADE_MAP = {\n",
        "    \"A+\": 100, \"A\": 95, \"A-\": 90,\n",
        "    \"B+\": 87,  \"B\": 83, \"B-\": 80,\n",
        "    \"C+\": 77,  \"C\": 73, \"C-\": 70,\n",
        "    \"D+\": 67,  \"D\": 63, \"D-\": 60,\n",
        "    \"F\": 50\n",
        "}\n",
        "\n",
        "def score_to_0_100(x):\n",
        "    \"\"\"Parsuje różne formaty → 0-100\"\"\"\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "\n",
        "    s = str(x).strip()\n",
        "\n",
        "    # 1) Procenty: \"84%\"\n",
        "    m = re.match(r\"^(\\d+(?:\\.\\d+)?)\\s*%$\", s)\n",
        "    if m:\n",
        "        return float(m.group(1))\n",
        "\n",
        "    # 2) Ułamek: \"3/5\", \"8/10\"\n",
        "    m = re.match(r\"^(\\d+(?:\\.\\d+)?)\\s*/\\s*(\\d+(?:\\.\\d+)?)$\", s)\n",
        "    if m:\n",
        "        num, den = float(m.group(1)), float(m.group(2))\n",
        "        if den > 0:\n",
        "            return 100.0 * num / den\n",
        "\n",
        "    # 3) Liczba (zakładamy /5 jeśli ≤5, /100 jeśli ≤100)\n",
        "    m = re.match(r\"^(\\d+(?:\\.\\d+)?)$\", s)\n",
        "    if m:\n",
        "        num = float(m.group(1))\n",
        "        if num <= 5:\n",
        "            return 100.0 * num / 5.0\n",
        "        elif num <= 10:  # już skala 1-10\n",
        "            return num * 10.0\n",
        "        elif num <= 100:\n",
        "            return num\n",
        "\n",
        "    # 4) Oceny literowe\n",
        "    if s.upper().strip() in GRADE_MAP:\n",
        "        return float(GRADE_MAP[s.upper().strip()])\n",
        "\n",
        "    return np.nan\n",
        "\n",
        "# KROK 1: Parsuj do 0-100\n",
        "df[\"score_100\"] = df[\"review_score\"].apply(score_to_0_100)\n",
        "\n",
        "# KROK 2: Przekształć 0-100 → 1-10 (CAŁKOWITE)\n",
        "df[\"score_10\"] = (df[\"score_100\"] / 10.0).round().clip(1, 10).astype('Int64')\n",
        "\n",
        "# Stats\n",
        "original_count = df[\"review_score\"].notna().sum()\n",
        "parsed_count = df[\"score_10\"].notna().sum()\n",
        "success_rate = (parsed_count / original_count * 100) if original_count > 0 else 0\n",
        "\n",
        "print(f\"✅ Parsing complete:\")\n",
        "print(f\"   Original review_score: {original_count}\")\n",
        "print(f\"   Parsed to score_10: {parsed_count}\")\n",
        "print(f\"   Success rate: {success_rate:.1f}%\")\n",
        "\n",
        "print(f\"\\n📊 Score distribution (1-10 scale):\")\n",
        "print(df[\"score_10\"].describe())\n",
        "print(f\"\\n📊 Value counts:\")\n",
        "print(df[\"score_10\"].value_counts().sort_index())\n",
        "\n",
        "# Histogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Skala 0-100\n",
        "df[\"score_100\"].dropna().hist(bins=50, ax=axes[0], edgecolor='black', color='#4ECDC4')\n",
        "axes[0].set_xlabel(\"Score (0-100)\", fontsize=12)\n",
        "axes[0].set_ylabel(\"Count\", fontsize=12)\n",
        "axes[0].set_title(\"Original Scale (0-100)\", fontsize=13, fontweight='bold')\n",
        "axes[0].axvline(df[\"score_100\"].median(), color='red', linestyle='--',\n",
        "                label=f'Median: {df[\"score_100\"].median():.0f}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Skala 1-10 (CAŁKOWITE)\n",
        "df[\"score_10\"].dropna().hist(bins=10, range=(0.5, 10.5), ax=axes[1],\n",
        "                              edgecolor='black', color='#45B7D1', align='mid')\n",
        "axes[1].set_xlabel(\"Score (1-10)\", fontsize=12)\n",
        "axes[1].set_ylabel(\"Count\", fontsize=12)\n",
        "axes[1].set_title(\"Transformed Scale (1-10, Integer)\", fontsize=13, fontweight='bold')\n",
        "axes[1].axvline(df[\"score_10\"].median(), color='red', linestyle='--',\n",
        "                label=f'Median: {df[\"score_10\"].median():.0f}')\n",
        "axes[1].legend()\n",
        "axes[1].set_xticks(range(1, 11))\n",
        "axes[1].set_xlim(0.5, 10.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7o3IPIMSStFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# ANALIZA N-GRAMÓW I LOG-ODDS\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"N-GRAM ANALYSIS: LOG-ODDS FOR REVIEW SCORES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Przygotuj dane (tylko te z review_score)\n",
        "df_for_ngrams = df[\n",
        "    df[\"score_10\"].notna() &\n",
        "    (df[\"text_cleaned\"].str.strip() != \"\")\n",
        "].copy()\n",
        "\n",
        "print(f\"\\n📊 Analyzing {len(df_for_ngrams)} reviews with valid scores\")\n",
        "print(f\"Score range: {df_for_ngrams['score_10'].min():.0f} - {df_for_ngrams['score_10'].max():.0f}\")\n",
        "\n",
        "# Wytrenuj prosty model Ridge na TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "vectorizer_ngrams = TfidfVectorizer(\n",
        "    ngram_range=(1, 3),      # unigramy + bigramy + trigramy\n",
        "    min_df=10,               # minimum 10 wystąpień (filtruje rzadkie)\n",
        "    max_features=20000,      # top 20k najczęstszych\n",
        "    sublinear_tf=True        # log-scaling TF\n",
        ")\n",
        "\n",
        "X_ngrams = vectorizer_ngrams.fit_transform(df_for_ngrams[\"text_cleaned\"])\n",
        "y_ngrams = df_for_ngrams[\"score_10\"].astype(float)\n",
        "\n",
        "# Prosty Ridge (bez split, bo to tylko analiza)\n",
        "ridge_ngrams = Ridge(alpha=1.0)\n",
        "ridge_ngrams.fit(X_ngrams, y_ngrams)\n",
        "\n",
        "# Wyciągnij feature names i współczynniki\n",
        "feature_names = vectorizer_ngrams.get_feature_names_out()\n",
        "coefficients = ridge_ngrams.coef_\n",
        "\n",
        "# Stwórz DataFrame: n-gram → współczynnik\n",
        "ngram_weights = pd.DataFrame({\n",
        "    'ngram': feature_names,\n",
        "    'log_odds': coefficients\n",
        "})\n",
        "\n",
        "# Posortuj\n",
        "ngram_weights = ngram_weights.sort_values('log_odds', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🔺 TOP 30 N-GRAMS BOOSTING SCORE (positive log-odds)\")\n",
        "print(\"=\"*60)\n",
        "print(ngram_weights.head(30).to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"🔻 TOP 30 N-GRAMS LOWERING SCORE (negative log-odds)\")\n",
        "print(\"=\"*60)\n",
        "print(ngram_weights.tail(30).to_string(index=False))\n",
        "\n",
        "# Opcjonalnie: wizualizacja top 20 każdego\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Top pozytywne\n",
        "top_positive = ngram_weights.head(20)\n",
        "axes[0].barh(top_positive['ngram'], top_positive['log_odds'], color='#4ECDC4')\n",
        "axes[0].set_xlabel('Log-Odds (coefficient)', fontsize=12)\n",
        "axes[0].set_title('Top 20 N-grams Increasing Score', fontsize=14, fontweight='bold')\n",
        "axes[0].invert_yaxis()\n",
        "\n",
        "# Top negatywne\n",
        "top_negative = ngram_weights.tail(20).sort_values('log_odds')\n",
        "axes[1].barh(top_negative['ngram'], top_negative['log_odds'], color='#FF6B6B')\n",
        "axes[1].set_xlabel('Log-Odds (coefficient)', fontsize=12)\n",
        "axes[1].set_title('Top 20 N-grams Decreasing Score', fontsize=14, fontweight='bold')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ Analysis complete. Total features: {len(feature_names)}\")\n"
      ],
      "metadata": {
        "id": "ziMv3lDGSduu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# TRENING MODELU: TEXT → SCORE (1-10)\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINING REGRESSION MODEL: TEXT → SCORE (1-10)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Filtruj dane\n",
        "df_with_score = df[\n",
        "    df[\"score_10\"].notna() &\n",
        "    (df[\"text_cleaned\"].str.strip() != \"\")\n",
        "].copy()\n",
        "\n",
        "print(f\"📊 Dataset: {len(df_with_score)} reviews\")\n",
        "print(f\"   Score range: {df_with_score['score_10'].min():.1f} - {df_with_score['score_10'].max():.1f}\")\n",
        "\n",
        "X = df_with_score[\"text_cleaned\"]\n",
        "y = df_with_score[\"score_10\"]  # ← ZMIANA: używamy skali 1-10\n",
        "\n",
        "# Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"   Train: {len(X_train)} | Test: {len(X_test)}\")\n",
        "\n",
        "# Model\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "regressor = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=30000)),\n",
        "    (\"ridge\", Ridge(alpha=1.0))  # może zmniejsz alpha dla mniejszej skali\n",
        "])\n",
        "\n",
        "print(\"\\n🚀 Training...\")\n",
        "regressor.fit(X_train, y_train)\n",
        "print(\"✅ Complete\")\n",
        "\n",
        "# Predykcje (clip do 1-10, round do 1 miejsca po przecinku)\n",
        "pred_train = regressor.predict(X_train).clip(1, 10).round(1)\n",
        "pred_test = regressor.predict(X_test).clip(1, 10).round(1)\n",
        "\n",
        "# Metryki\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "mae_train = mean_absolute_error(y_train, pred_train)\n",
        "mae_test = mean_absolute_error(y_test, pred_test)\n",
        "rmse_test = mean_squared_error(y_test, pred_test) ** 0.5\n",
        "r2_test = r2_score(y_test, pred_test)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"📊 MODEL PERFORMANCE (1-10 scale)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Train MAE: {mae_train:.3f} points\")\n",
        "print(f\"Test MAE:  {mae_test:.3f} points\")\n",
        "print(f\"Test RMSE: {rmse_test:.3f} points\")\n",
        "print(f\"Test R²:   {r2_test:.3f}\")\n",
        "\n",
        "# Accuracy-like (tolerance na skali 1-10)\n",
        "within_05 = (np.abs(y_test - pred_test) <= 0.5).mean()\n",
        "within_10 = (np.abs(y_test - pred_test) <= 1.0).mean()\n",
        "within_15 = (np.abs(y_test - pred_test) <= 1.5).mean()\n",
        "\n",
        "print(f\"\\n📏 Prediction Tolerance:\")\n",
        "print(f\"   Within ±0.5 points: {within_05:.1%}\")\n",
        "print(f\"   Within ±1.0 points: {within_10:.1%}\")\n",
        "print(f\"   Within ±1.5 points: {within_15:.1%}\")\n",
        "\n",
        "# Scatter plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "axes[0].scatter(y_test, pred_test, alpha=0.4, s=15, color='#4ECDC4')\n",
        "axes[0].plot([1, 10], [1, 10], 'r--', lw=2, label='Perfect prediction')\n",
        "axes[0].set_xlabel(\"Actual Score (1-10)\", fontsize=12)\n",
        "axes[0].set_ylabel(\"Predicted Score (1-10)\", fontsize=12)\n",
        "axes[0].set_title(\"Predicted vs Actual\", fontsize=13, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].set_xlim(0.5, 10.5)\n",
        "axes[0].set_ylim(0.5, 10.5)\n",
        "\n",
        "# Residuals\n",
        "residuals = y_test - pred_test\n",
        "axes[1].hist(residuals, bins=40, edgecolor='black', color='#FF6B6B')\n",
        "axes[1].set_xlabel(\"Residual (Actual - Predicted)\", fontsize=12)\n",
        "axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
        "axes[1].set_title(\"Prediction Errors\", fontsize=13, fontweight='bold')\n",
        "axes[1].axvline(0, color='black', linestyle='--', linewidth=2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n✅ Model trained on 1-10 scale\")\n"
      ],
      "metadata": {
        "id": "McRiNY2_NPfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# PREDYKCJA DLA CAŁEGO DATASETU\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERATING PREDICTIONS (1-10 scale)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Predykcja\n",
        "df[\"predicted_score_10\"] = np.nan\n",
        "mask = df[\"text_cleaned\"].notna() & (df[\"text_cleaned\"].str.strip() != \"\")\n",
        "\n",
        "df.loc[mask, \"predicted_score_10\"] = (\n",
        "    regressor.predict(df.loc[mask, \"text_cleaned\"])\n",
        "    .clip(1, 10)\n",
        "    .round(1)\n",
        ")\n",
        "\n",
        "print(f\"✅ Predictions for {mask.sum()} reviews\")\n",
        "print(f\"\\n📊 Predicted score distribution:\")\n",
        "print(df[\"predicted_score_10\"].describe())\n",
        "\n",
        "# Residuals\n",
        "df[\"score_residual\"] = df[\"score_10\"] - df[\"predicted_score_10\"]\n",
        "\n",
        "print(f\"\\n📊 Residuals:\")\n",
        "print(df[\"score_residual\"].describe())\n",
        "\n",
        "# Możesz też skonwertować z powrotem do 0-100 dla wizualizacji\n",
        "df[\"predicted_score_100\"] = (df[\"predicted_score_10\"] * 10).round().astype(\"Int64\")\n",
        "\n",
        "print(\"\\n✅ Available columns:\")\n",
        "print(\"   - score_10 (target, 1-10)\")\n",
        "print(\"   - predicted_score_10 (model output, 1-10)\")\n",
        "print(\"   - predicted_score_100 (converted back, 0-100)\")\n",
        "print(\"   - score_residual (error)\")\n"
      ],
      "metadata": {
        "id": "B1pCCe4xNYte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LISTA ZMIAN PO PREDYKCJI OCEN\n",
        "\n",
        "- Skalowanie na 0-100 zostawić jako cecha do wizualizacji, aniżeli cecha do predykcji w modelu nr.3\n",
        "- zastanowić się nad określaniem czy znikoma różnica między ocenami recenzenta to dobra rzecz"
      ],
      "metadata": {
        "id": "EFRaOUfyruVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# detekcja większej ilości recenzji w trakcie 24h recenzenta - raczej nie powinien np wystawić 20 recenzji na dzień\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CALCULATING REVIEWER BURST DETECTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def calculate_reviewer_burst(group):\n",
        "    \"\"\"\n",
        "    Wykrywa burst activity dla pojedynczego recenzenta.\n",
        "    Podejrzane: wiele recenzji w krótkim czasie.\n",
        "    \"\"\"\n",
        "    group = group.sort_values('review_date').copy()\n",
        "    total_reviews = len(group)\n",
        "\n",
        "    counts_24h = []\n",
        "    counts_7d = []\n",
        "\n",
        "    for idx, row in group.iterrows():\n",
        "        current_time = row['review_date']\n",
        "\n",
        "        # Okno 24h\n",
        "        window_24h_start = current_time - timedelta(hours=24)\n",
        "        count_24h = ((group['review_date'] >= window_24h_start) &\n",
        "                     (group['review_date'] <= current_time)).sum()\n",
        "        counts_24h.append(count_24h)\n",
        "\n",
        "        # Okno 7 dni\n",
        "        window_7d_start = current_time - timedelta(days=7)\n",
        "        count_7d = ((group['review_date'] >= window_7d_start) &\n",
        "                    (group['review_date'] <= current_time)).sum()\n",
        "        counts_7d.append(count_7d)\n",
        "\n",
        "    group['reviewer_reviews_last_24h'] = counts_24h\n",
        "    group['reviewer_reviews_last_7d'] = counts_7d\n",
        "\n",
        "    # Normalizacja\n",
        "    group['reviewer_burst_ratio_24h'] = group['reviewer_reviews_last_24h'] / total_reviews\n",
        "\n",
        "    return group\n",
        "\n",
        "# Zastosuj per reviewer\n",
        "print(\"🔄 Processing burst detection per reviewer...\")\n",
        "df = df.groupby('critic_name', group_keys=False).apply(\n",
        "    calculate_reviewer_burst\n",
        ")\n",
        "\n",
        "# ========================================\n",
        "# KROK 2: Flagi burst dla recenzentów\n",
        "# ========================================\n",
        "\n",
        "# Thresholdy - dostosuj do swoich danych\n",
        "df['is_reviewer_burst_24h'] = df['reviewer_reviews_last_24h'] > 20  # >20 recenzji w 24h\n",
        "df['is_reviewer_burst_7d'] = df['reviewer_reviews_last_7d'] > 50     # >50 recenzji w tydzień\n",
        "\n",
        "# Normalized (np. >30% wszystkich recenzji w 24h)\n",
        "df['is_reviewer_burst_normalized'] = df['reviewer_burst_ratio_24h'] > 0.3\n",
        "\n",
        "# Combined\n",
        "df['is_reviewer_burst_any'] = (\n",
        "    df['is_reviewer_burst_24h'] |\n",
        "    df['is_reviewer_burst_7d'] |\n",
        "    df['is_reviewer_burst_normalized']\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ Reviewer burst detection complete:\")\n",
        "print(f\"   Burst 24h (>20 reviews): {df['is_reviewer_burst_24h'].sum()} reviews ({df['is_reviewer_burst_24h'].mean()*100:.1f}%)\")\n",
        "print(f\"   Burst 7d (>50 reviews): {df['is_reviewer_burst_7d'].sum()} reviews ({df['is_reviewer_burst_7d'].mean()*100:.1f}%)\")\n",
        "print(f\"   Normalized burst (>30% in 24h): {df['is_reviewer_burst_normalized'].sum()} reviews ({df['is_reviewer_burst_normalized'].mean()*100:.1f}%)\")\n",
        "print(f\"   Any burst: {df['is_reviewer_burst_any'].sum()} reviews ({df['is_reviewer_burst_any'].mean()*100:.1f}%)\")\n",
        "\n",
        "# ========================================\n",
        "# KROK 3: Identyfikacja najgorszych recenzentów\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYZING BURSTY REVIEWERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Agreguj per reviewer\n",
        "bursty_reviewers = df[df['is_reviewer_burst_any']].groupby('critic_name').agg({\n",
        "    'reviewer_reviews_last_24h': 'max',\n",
        "    'reviewer_reviews_last_7d': 'max',\n",
        "    'reviewer_burst_ratio_24h': 'max',\n",
        "    'review_date': ['min', 'max'],\n",
        "    'review_content': 'count'\n",
        "}).reset_index()\n",
        "\n",
        "bursty_reviewers.columns = ['critic_name', 'max_burst_24h', 'max_burst_7d',\n",
        "                             'max_burst_ratio', 'first_review', 'last_review',\n",
        "                             'total_reviews']\n",
        "bursty_reviewers = bursty_reviewers.sort_values('max_burst_24h', ascending=False)\n",
        "\n",
        "print(f\"\\n👤 Reviewers with burst activity: {len(bursty_reviewers)}\")\n",
        "print(\"\\n🚨 TOP 20 Most Bursty Reviewers:\")\n",
        "print(bursty_reviewers.head(20).to_string(index=False))\n",
        "\n",
        "# ========================================\n",
        "# KROK 4: Wizualizacja konkretnego recenzenta\n",
        "# ========================================\n",
        "if len(bursty_reviewers) > 0:\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    example_reviewer = bursty_reviewers.iloc[0]['critic_name']\n",
        "    reviewer_data = df[df['critic_name'] == example_reviewer].sort_values('review_date')\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    fig.suptitle(f'Burst Analysis: {example_reviewer}', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Plot 1: Time series - reviews in last 24h\n",
        "    reviewer_data.set_index('review_date')['reviewer_reviews_last_24h'].plot(\n",
        "        ax=axes[0, 0], color='red', linewidth=2\n",
        "    )\n",
        "    axes[0, 0].axhline(y=20, color='black', linestyle='--',\n",
        "                       linewidth=2, label='Burst threshold (20)')\n",
        "    axes[0, 0].set_title('Reviews Published in Last 24h (Rolling)', fontweight='bold')\n",
        "    axes[0, 0].set_ylabel('Reviews in last 24h', fontsize=11)\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: Time series - reviews in last 7d\n",
        "    reviewer_data.set_index('review_date')['reviewer_reviews_last_7d'].plot(\n",
        "        ax=axes[0, 1], color='orange', linewidth=2\n",
        "    )\n",
        "    axes[0, 1].axhline(y=50, color='black', linestyle='--',\n",
        "                       linewidth=2, label='Burst threshold (50)')\n",
        "    axes[0, 1].set_title('Reviews Published in Last 7 Days (Rolling)', fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('Reviews in last 7 days', fontsize=11)\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Daily distribution\n",
        "    daily_counts = reviewer_data['review_date'].dt.date.value_counts().sort_index()\n",
        "    daily_counts.plot(kind='bar', ax=axes[1, 0], color='steelblue', width=0.8)\n",
        "    axes[1, 0].set_title('Daily Review Count', fontweight='bold')\n",
        "    axes[1, 0].set_ylabel('Number of Reviews', fontsize=11)\n",
        "    axes[1, 0].set_xlabel('Date', fontsize=11)\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Plot 4: Hourly distribution (if timestamps available)\n",
        "    if pd.api.types.is_datetime64_any_dtype(reviewer_data['review_date']):\n",
        "        hourly_counts = reviewer_data['review_date'].dt.hour.value_counts().sort_index()\n",
        "        hourly_counts.plot(kind='bar', ax=axes[1, 1], color='#4ECDC4', width=0.8)\n",
        "        axes[1, 1].set_title('Review Distribution by Hour of Day', fontweight='bold')\n",
        "        axes[1, 1].set_ylabel('Number of Reviews', fontsize=11)\n",
        "        axes[1, 1].set_xlabel('Hour (0-23)', fontsize=11)\n",
        "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Stats dla tego recenzenta\n",
        "    print(f\"\\n📊 Burst Analysis for '{example_reviewer}':\")\n",
        "    print(f\"   Total reviews: {len(reviewer_data)}\")\n",
        "    print(f\"   Peak 24h burst: {reviewer_data['reviewer_reviews_last_24h'].max()} reviews\")\n",
        "    print(f\"   Peak 7d burst: {reviewer_data['reviewer_reviews_last_7d'].max()} reviews\")\n",
        "    print(f\"   Max burst ratio: {reviewer_data['reviewer_burst_ratio_24h'].max()*100:.1f}%\")\n",
        "    print(f\"   Reviews in burst periods: {reviewer_data['is_reviewer_burst_any'].sum()}\")\n",
        "    print(f\"   Activity span: {(reviewer_data['review_date'].max() - reviewer_data['review_date'].min()).days} days\")\n",
        "\n",
        "# ========================================\n",
        "# KROK 5: Porównanie thresholds\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"REVIEWER BURST THRESHOLD COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "thresholds_24h = [5, 10, 15, 20, 30, 50, 100]\n",
        "thresholds_7d = [20, 30, 50, 75, 100, 150, 200]\n",
        "\n",
        "print(\"\\n24-hour window thresholds:\")\n",
        "print(f\"{'Threshold':<12} {'Reviews':<12} {'% of Reviews':<15} {'Reviewers':<12}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for thresh in thresholds_24h:\n",
        "    review_count = (df['reviewer_reviews_last_24h'] > thresh).sum()\n",
        "    review_pct = review_count / len(df) * 100\n",
        "    reviewer_count = df[df['reviewer_reviews_last_24h'] > thresh]['critic_name'].nunique()\n",
        "\n",
        "    print(f\">{thresh:<11} {review_count:<12} {review_pct:<15.2f}% {reviewer_count:<12}\")\n",
        "\n",
        "print(\"\\n7-day window thresholds:\")\n",
        "print(f\"{'Threshold':<12} {'Reviews':<12} {'% of Reviews':<15} {'Reviewers':<12}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for thresh in thresholds_7d:\n",
        "    review_count = (df['reviewer_reviews_last_7d'] > thresh).sum()\n",
        "    review_pct = review_count / len(df) * 100\n",
        "    reviewer_count = df[df['reviewer_reviews_last_7d'] > thresh]['critic_name'].nunique()\n",
        "\n",
        "    print(f\">{thresh:<11} {review_count:<12} {review_pct:<15.2f}% {reviewer_count:<12}\")\n",
        "\n",
        "# ========================================\n",
        "# KROK 6: Statystyki ogólne\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"REVIEWER BURST STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n📈 24h burst intensity distribution:\")\n",
        "print(f\"   Mean: {df['reviewer_reviews_last_24h'].mean():.2f}\")\n",
        "print(f\"   Median: {df['reviewer_reviews_last_24h'].median():.1f}\")\n",
        "print(f\"   75th percentile: {df['reviewer_reviews_last_24h'].quantile(0.75):.1f}\")\n",
        "print(f\"   95th percentile: {df['reviewer_reviews_last_24h'].quantile(0.95):.1f}\")\n",
        "print(f\"   99th percentile: {df['reviewer_reviews_last_24h'].quantile(0.99):.1f}\")\n",
        "print(f\"   Max: {df['reviewer_reviews_last_24h'].max():.0f}\")\n",
        "\n",
        "print(\"\\n📈 7-day burst intensity distribution:\")\n",
        "print(f\"   Mean: {df['reviewer_reviews_last_7d'].mean():.2f}\")\n",
        "print(f\"   Median: {df['reviewer_reviews_last_7d'].median():.1f}\")\n",
        "print(f\"   75th percentile: {df['reviewer_reviews_last_7d'].quantile(0.75):.1f}\")\n",
        "print(f\"   95th percentile: {df['reviewer_reviews_last_7d'].quantile(0.95):.1f}\")\n",
        "print(f\"   99th percentile: {df['reviewer_reviews_last_7d'].quantile(0.99):.1f}\")\n",
        "print(f\"   Max: {df['reviewer_reviews_last_7d'].max():.0f}\")\n",
        "\n",
        "# Histogramy\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 24h bursts\n",
        "df['reviewer_reviews_last_24h'].hist(bins=50, ax=axes[0], color='#FF6B6B', edgecolor='black')\n",
        "axes[0].axvline(20, color='black', linestyle='--', linewidth=2, label='Threshold: 20')\n",
        "axes[0].set_xlabel('Reviews by Reviewer in 24h', fontsize=12)\n",
        "axes[0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[0].set_title('Reviewer 24h Burst Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].set_yscale('log')\n",
        "\n",
        "# 7d bursts\n",
        "df['reviewer_reviews_last_7d'].hist(bins=50, ax=axes[1], color='#FFA07A', edgecolor='black')\n",
        "axes[1].axvline(50, color='black', linestyle='--', linewidth=2, label='Threshold: 50')\n",
        "axes[1].set_xlabel('Reviews by Reviewer in 7 Days', fontsize=12)\n",
        "axes[1].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1].set_title('Reviewer 7-Day Burst Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "6WSx9DKvJ_V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statystyki recenzentów\n",
        "\n",
        "# 1. Podstawowe statystyki recenzenta\n",
        "reviewer_stats = df.groupby('critic_name').agg({\n",
        "    'review_date': ['count', 'min', 'max'],  # ile recenzji, zakres dat\n",
        "    'review_type': lambda x: (x == 'Fresh').mean(),  # % pozytywnych\n",
        "    'movie_title': 'nunique',  # ile różnych filmów\n",
        "    'score_10': ['mean', 'std'],  # średnia ocena, wariancja\n",
        "    'text_length_cleaned': 'mean'  # średnia długość tekstu\n",
        "}).reset_index()\n",
        "\n",
        "reviewer_stats.columns = ['critic_name', 'review_count', 'first_review',\n",
        "                          'last_review', 'positive_ratio', 'unique_movies',\n",
        "                          'avg_score', 'score_std', 'avg_text_length']\n",
        "\n",
        "# 2. Reviewer deviation (web:12) - jak różni się od średniej\n",
        "reviewer_stats['deviation_from_mean'] = np.abs(\n",
        "    reviewer_stats['avg_score'] - df['score_10'].mean()\n",
        ")\n",
        "\n",
        "# 3. Activity span\n",
        "reviewer_stats['days_active'] = (\n",
        "    reviewer_stats['last_review'] - reviewer_stats['first_review']\n",
        ").dt.days\n",
        "\n",
        "# 4. Suspicious patterns\n",
        "reviewer_stats['is_suspicious'] = (\n",
        "    (reviewer_stats['review_count'] < 5) |  # mało recenzji\n",
        "    (reviewer_stats['positive_ratio'] > 0.95) |  # tylko pozytywne\n",
        "    (reviewer_stats['positive_ratio'] < 0.05) |  # tylko negatywne\n",
        "    (reviewer_stats['days_active'] < 30)  # krótki okres aktywności\n",
        ")\n",
        "\n",
        "# 5. Merge z głównym df\n",
        "df = df.merge(reviewer_stats, on='critic_name', how='left')"
      ],
      "metadata": {
        "id": "zViZWxXzRgkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# ========================================\n",
        "# SETUP\n",
        "# ========================================\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (16, 10)\n",
        "\n",
        "# ========================================\n",
        "# VIZ 1: OVERVIEW - Podstawowe metryki recenzentów\n",
        "# ========================================\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Reviewer Statistics Overview', fontsize=16, fontweight='bold', y=0.995)\n",
        "\n",
        "# 1.1 Review count distribution\n",
        "axes[0, 0].hist(reviewer_stats['review_count'], bins=50, color='#4ECDC4', edgecolor='black')\n",
        "axes[0, 0].axvline(reviewer_stats['review_count'].median(), color='red',\n",
        "                   linestyle='--', label=f\"Median: {reviewer_stats['review_count'].median():.0f}\")\n",
        "axes[0, 0].set_xlabel('Number of Reviews', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Number of Reviewers', fontsize=11)\n",
        "axes[0, 0].set_title('Distribution of Review Count per Reviewer', fontweight='bold')\n",
        "axes[0, 0].set_yscale('log')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# 1.2 Positive ratio distribution\n",
        "axes[0, 1].hist(reviewer_stats['positive_ratio'], bins=30, color='#45B7D1', edgecolor='black')\n",
        "axes[0, 1].axvline(0.5, color='green', linestyle='--', label='Balanced (50%)')\n",
        "axes[0, 1].axvline(0.95, color='red', linestyle='--', label='Suspicious (>95%)')\n",
        "axes[0, 1].axvline(0.05, color='red', linestyle='--', label='Suspicious (<5%)')\n",
        "axes[0, 1].set_xlabel('Positive Review Ratio', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Number of Reviewers', fontsize=11)\n",
        "axes[0, 1].set_title('Fresh/Rotten Ratio Distribution', fontweight='bold')\n",
        "axes[0, 1].legend(fontsize=9)\n",
        "\n",
        "# 1.3 Days active distribution\n",
        "axes[0, 2].hist(reviewer_stats['days_active'], bins=50, color='#FF6B6B', edgecolor='black')\n",
        "axes[0, 2].axvline(30, color='red', linestyle='--', label='Suspicious (<30 days)')\n",
        "axes[0, 2].set_xlabel('Days Active', fontsize=11)\n",
        "axes[0, 2].set_ylabel('Number of Reviewers', fontsize=11)\n",
        "axes[0, 2].set_title('Reviewer Activity Span', fontweight='bold')\n",
        "axes[0, 2].legend()\n",
        "\n",
        "# 1.4 Average score distribution\n",
        "axes[1, 0].hist(reviewer_stats['avg_score'].dropna(), bins=30, color='#95E1D3', edgecolor='black')\n",
        "axes[1, 0].axvline(reviewer_stats['avg_score'].mean(), color='red',\n",
        "                   linestyle='--', label=f\"Mean: {reviewer_stats['avg_score'].mean():.2f}\")\n",
        "axes[1, 0].set_xlabel('Average Score (1-10)', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Number of Reviewers', fontsize=11)\n",
        "axes[1, 0].set_title('Average Reviewer Score Distribution', fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 1.5 Score standard deviation\n",
        "axes[1, 1].hist(reviewer_stats['score_std'].dropna(), bins=30, color='#F38181', edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Score Standard Deviation', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Number of Reviewers', fontsize=11)\n",
        "axes[1, 1].set_title('Score Variability per Reviewer', fontweight='bold')\n",
        "\n",
        "# 1.6 Unique movies reviewed\n",
        "axes[1, 2].hist(reviewer_stats['unique_movies'], bins=50, color='#AA96DA', edgecolor='black')\n",
        "axes[1, 2].set_xlabel('Number of Unique Movies', fontsize=11)\n",
        "axes[1, 2].set_ylabel('Number of Reviewers', fontsize=11)\n",
        "axes[1, 2].set_title('Movies Reviewed per Critic', fontweight='bold')\n",
        "axes[1, 2].set_yscale('log')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# VIZ 2: SUSPICIOUS vs NORMAL Comparison\n",
        "# ========================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Suspicious vs Normal Reviewers Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "suspicious = reviewer_stats[reviewer_stats['is_suspicious']]\n",
        "normal = reviewer_stats[~reviewer_stats['is_suspicious']]\n",
        "\n",
        "print(f\"\\n📊 Reviewer Classification:\")\n",
        "print(f\"   Suspicious reviewers: {len(suspicious)} ({len(suspicious)/len(reviewer_stats)*100:.1f}%)\")\n",
        "print(f\"   Normal reviewers: {len(normal)} ({len(normal)/len(reviewer_stats)*100:.1f}%)\")\n",
        "\n",
        "# 2.1 Review count comparison\n",
        "data_to_plot = [normal['review_count'], suspicious['review_count']]\n",
        "bp1 = axes[0, 0].boxplot(data_to_plot, labels=['Normal', 'Suspicious'],\n",
        "                          patch_artist=True, widths=0.6)\n",
        "bp1['boxes'][0].set_facecolor('#4ECDC4')\n",
        "bp1['boxes'][1].set_facecolor('#FF6B6B')\n",
        "axes[0, 0].set_ylabel('Review Count', fontsize=11)\n",
        "axes[0, 0].set_title('Review Count: Normal vs Suspicious', fontweight='bold')\n",
        "axes[0, 0].set_yscale('log')\n",
        "\n",
        "# 2.2 Positive ratio comparison\n",
        "data_to_plot = [normal['positive_ratio'], suspicious['positive_ratio']]\n",
        "bp2 = axes[0, 1].boxplot(data_to_plot, labels=['Normal', 'Suspicious'],\n",
        "                          patch_artist=True, widths=0.6)\n",
        "bp2['boxes'][0].set_facecolor('#4ECDC4')\n",
        "bp2['boxes'][1].set_facecolor('#FF6B6B')\n",
        "axes[0, 1].set_ylabel('Positive Ratio', fontsize=11)\n",
        "axes[0, 1].set_title('Fresh Ratio: Normal vs Suspicious', fontweight='bold')\n",
        "axes[0, 1].axhline(0.5, color='green', linestyle='--', alpha=0.5)\n",
        "\n",
        "# 2.3 Days active comparison\n",
        "data_to_plot = [normal['days_active'], suspicious['days_active']]\n",
        "bp3 = axes[1, 0].boxplot(data_to_plot, labels=['Normal', 'Suspicious'],\n",
        "                          patch_artist=True, widths=0.6)\n",
        "bp3['boxes'][0].set_facecolor('#4ECDC4')\n",
        "bp3['boxes'][1].set_facecolor('#FF6B6B')\n",
        "axes[1, 0].set_ylabel('Days Active', fontsize=11)\n",
        "axes[1, 0].set_title('Activity Span: Normal vs Suspicious', fontweight='bold')\n",
        "\n",
        "# 2.4 Score deviation comparison\n",
        "data_to_plot = [normal['deviation_from_mean'].dropna(), suspicious['deviation_from_mean'].dropna()]\n",
        "bp4 = axes[1, 1].boxplot(data_to_plot, labels=['Normal', 'Suspicious'],\n",
        "                          patch_artist=True, widths=0.6)\n",
        "bp4['boxes'][0].set_facecolor('#4ECDC4')\n",
        "bp4['boxes'][1].set_facecolor('#FF6B6B')\n",
        "axes[1, 1].set_ylabel('Score Deviation from Mean', fontsize=11)\n",
        "axes[1, 1].set_title('Score Deviation: Normal vs Suspicious', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# VIZ 3: SCATTER PLOTS - Relationships\n",
        "# ========================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "fig.suptitle('Reviewer Behavior Patterns', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 3.1 Review count vs Positive ratio\n",
        "axes[0, 0].scatter(normal['review_count'], normal['positive_ratio'],\n",
        "                   alpha=0.5, s=30, color='#4ECDC4', label='Normal')\n",
        "axes[0, 0].scatter(suspicious['review_count'], suspicious['positive_ratio'],\n",
        "                   alpha=0.7, s=50, color='#FF6B6B', marker='x', label='Suspicious')\n",
        "axes[0, 0].axhline(0.95, color='red', linestyle='--', alpha=0.3)\n",
        "axes[0, 0].axhline(0.05, color='red', linestyle='--', alpha=0.3)\n",
        "axes[0, 0].axvline(5, color='red', linestyle='--', alpha=0.3)\n",
        "axes[0, 0].set_xlabel('Review Count', fontsize=11)\n",
        "axes[0, 0].set_ylabel('Positive Ratio', fontsize=11)\n",
        "axes[0, 0].set_title('Review Count vs Sentiment Bias', fontweight='bold')\n",
        "axes[0, 0].set_xscale('log')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 3.2 Days active vs Review count\n",
        "axes[0, 1].scatter(normal['days_active'], normal['review_count'],\n",
        "                   alpha=0.5, s=30, color='#4ECDC4', label='Normal')\n",
        "axes[0, 1].scatter(suspicious['days_active'], suspicious['review_count'],\n",
        "                   alpha=0.7, s=50, color='#FF6B6B', marker='x', label='Suspicious')\n",
        "axes[0, 1].axvline(30, color='red', linestyle='--', alpha=0.3, label='30-day threshold')\n",
        "axes[0, 1].set_xlabel('Days Active', fontsize=11)\n",
        "axes[0, 1].set_ylabel('Review Count', fontsize=11)\n",
        "axes[0, 1].set_title('Activity Duration vs Volume', fontweight='bold')\n",
        "axes[0, 1].set_yscale('log')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3.3 Score std vs avg score\n",
        "axes[1, 0].scatter(normal['avg_score'], normal['score_std'],\n",
        "                   alpha=0.5, s=30, color='#4ECDC4', label='Normal')\n",
        "axes[1, 0].scatter(suspicious['avg_score'], suspicious['score_std'],\n",
        "                   alpha=0.7, s=50, color='#FF6B6B', marker='x', label='Suspicious')\n",
        "axes[1, 0].set_xlabel('Average Score', fontsize=11)\n",
        "axes[1, 0].set_ylabel('Score Standard Deviation', fontsize=11)\n",
        "axes[1, 0].set_title('Score Consistency Pattern', fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 3.4 Review count vs unique movies\n",
        "axes[1, 1].scatter(normal['unique_movies'], normal['review_count'],\n",
        "                   alpha=0.5, s=30, color='#4ECDC4', label='Normal')\n",
        "axes[1, 1].scatter(suspicious['unique_movies'], suspicious['review_count'],\n",
        "                   alpha=0.7, s=50, color='#FF6B6B', marker='x', label='Suspicious')\n",
        "# Idealna linia: 1 review = 1 movie\n",
        "max_val = max(reviewer_stats['unique_movies'].max(), reviewer_stats['review_count'].max())\n",
        "axes[1, 1].plot([0, max_val], [0, max_val], 'g--', alpha=0.3, label='1:1 ratio')\n",
        "axes[1, 1].set_xlabel('Unique Movies Reviewed', fontsize=11)\n",
        "axes[1, 1].set_ylabel('Total Review Count', fontsize=11)\n",
        "axes[1, 1].set_title('Review Diversity Pattern', fontweight='bold')\n",
        "axes[1, 1].set_xscale('log')\n",
        "axes[1, 1].set_yscale('log')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# VIZ 4: TOP/BOTTOM Reviewers\n",
        "# ========================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "fig.suptitle('Extreme Reviewers Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 4.1 Top 20 most active reviewers\n",
        "top_active = reviewer_stats.nlargest(20, 'review_count')\n",
        "axes[0, 0].barh(range(len(top_active)), top_active['review_count'], color='#4ECDC4')\n",
        "axes[0, 0].set_yticks(range(len(top_active)))\n",
        "axes[0, 0].set_yticklabels(top_active['critic_name'], fontsize=8)\n",
        "axes[0, 0].set_xlabel('Review Count', fontsize=11)\n",
        "axes[0, 0].set_title('Top 20 Most Active Reviewers', fontweight='bold')\n",
        "axes[0, 0].invert_yaxis()\n",
        "\n",
        "# 4.2 Most biased positive\n",
        "most_positive = reviewer_stats[reviewer_stats['review_count'] >= 10].nlargest(20, 'positive_ratio')\n",
        "axes[0, 1].barh(range(len(most_positive)), most_positive['positive_ratio']*100, color='#00EA6E')\n",
        "axes[0, 1].set_yticks(range(len(most_positive)))\n",
        "axes[0, 1].set_yticklabels(most_positive['critic_name'], fontsize=8)\n",
        "axes[0, 1].set_xlabel('Fresh Review % (min 10 reviews)', fontsize=11)\n",
        "axes[0, 1].set_title('Top 20 Most Positive Reviewers', fontweight='bold')\n",
        "axes[0, 1].invert_yaxis()\n",
        "\n",
        "# 4.3 Most biased negative\n",
        "most_negative = reviewer_stats[reviewer_stats['review_count'] >= 10].nsmallest(20, 'positive_ratio')\n",
        "axes[1, 0].barh(range(len(most_negative)), most_negative['positive_ratio']*100, color='#FF3B3F')\n",
        "axes[1, 0].set_yticks(range(len(most_negative)))\n",
        "axes[1, 0].set_yticklabels(most_negative['critic_name'], fontsize=8)\n",
        "axes[1, 0].set_xlabel('Fresh Review % (min 10 reviews)', fontsize=11)\n",
        "axes[1, 0].set_title('Top 20 Most Negative Reviewers', fontweight='bold')\n",
        "axes[1, 0].invert_yaxis()\n",
        "\n",
        "# 4.4 Highest deviation from mean score\n",
        "top_deviation = reviewer_stats[reviewer_stats['review_count'] >= 10].nlargest(20, 'deviation_from_mean')\n",
        "axes[1, 1].barh(range(len(top_deviation)), top_deviation['deviation_from_mean'], color='#F38181')\n",
        "axes[1, 1].set_yticks(range(len(top_deviation)))\n",
        "axes[1, 1].set_yticklabels(top_deviation['critic_name'], fontsize=8)\n",
        "axes[1, 1].set_xlabel('Score Deviation from Mean (min 10 reviews)', fontsize=11)\n",
        "axes[1, 1].set_title('Top 20 Most Extreme Scorers', fontweight='bold')\n",
        "axes[1, 1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# VIZ 5: SUMMARY TABLE\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REVIEWER STATISTICS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "summary_data = {\n",
        "    'Metric': ['Total Reviewers', 'Suspicious Reviewers', 'Avg Reviews per Critic',\n",
        "               'Median Reviews', 'Avg Positive Ratio', 'Avg Days Active',\n",
        "               'Avg Unique Movies', 'Avg Score Deviation'],\n",
        "    'All Reviewers': [\n",
        "        len(reviewer_stats),\n",
        "        f\"{len(suspicious)} ({len(suspicious)/len(reviewer_stats)*100:.1f}%)\",\n",
        "        f\"{reviewer_stats['review_count'].mean():.1f}\",\n",
        "        f\"{reviewer_stats['review_count'].median():.0f}\",\n",
        "        f\"{reviewer_stats['positive_ratio'].mean()*100:.1f}%\",\n",
        "        f\"{reviewer_stats['days_active'].mean():.0f}\",\n",
        "        f\"{reviewer_stats['unique_movies'].mean():.1f}\",\n",
        "        f\"{reviewer_stats['deviation_from_mean'].mean():.2f}\"\n",
        "    ],\n",
        "    'Normal': [\n",
        "        len(normal),\n",
        "        \"-\",\n",
        "        f\"{normal['review_count'].mean():.1f}\",\n",
        "        f\"{normal['review_count'].median():.0f}\",\n",
        "        f\"{normal['positive_ratio'].mean()*100:.1f}%\",\n",
        "        f\"{normal['days_active'].mean():.0f}\",\n",
        "        f\"{normal['unique_movies'].mean():.1f}\",\n",
        "        f\"{normal['deviation_from_mean'].mean():.2f}\"\n",
        "    ],\n",
        "    'Suspicious': [\n",
        "        len(suspicious),\n",
        "        f\"{len(suspicious)} (100%)\",\n",
        "        f\"{suspicious['review_count'].mean():.1f}\",\n",
        "        f\"{suspicious['review_count'].median():.0f}\",\n",
        "        f\"{suspicious['positive_ratio'].mean()*100:.1f}%\",\n",
        "        f\"{suspicious['days_active'].mean():.0f}\",\n",
        "        f\"{suspicious['unique_movies'].mean():.1f}\",\n",
        "        f\"{suspicious['deviation_from_mean'].mean():.2f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "import pandas as pd\n",
        "summary_df = pd.DataFrame(summary_data)\n",
        "print(summary_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "AIPImIJEZHFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# SUSPICIOUS SCORE: Kompozytowa metryka podejrzliwości recenzenta\n",
        "# Skala: 1 (najmniej podejrzany) → 10 (najbardziej podejrzany)\n",
        "# ========================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_reviewer_suspicious_score(df):\n",
        "    \"\"\"\n",
        "    Oblicza suspicious_score dla każdego recenzenta (1-10)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Agreguj statystyki per recenzent\n",
        "    reviewer_stats = df.groupby('critic_name').agg({\n",
        "        'critic_name': 'count',                    # review_count\n",
        "        'review_type_encoded': 'mean',             # positive_ratio\n",
        "        'score_10': ['mean', 'std'],               # score stats\n",
        "        'is_reviewer_burst_24h': 'sum',            # burst count\n",
        "        'days_since_release': 'mean',              # avg timing\n",
        "    }).reset_index()\n",
        "\n",
        "    reviewer_stats.columns = ['critic_name', 'review_count', 'positive_ratio',\n",
        "                               'score_mean', 'score_std', 'burst_count', 'avg_days_since_release']\n",
        "\n",
        "    # 2. Oblicz sub-scores (każdy w skali 0-1)\n",
        "\n",
        "    # a) Burst behavior (0-1)\n",
        "    # Wysoki burst = podejrzane\n",
        "    burst_score = np.clip(reviewer_stats['burst_count'] / 5, 0, 1)\n",
        "\n",
        "    # b) Review velocity (0-1)\n",
        "    # Bardzo dużo recenzji = podejrzane\n",
        "    velocity_score = np.clip((reviewer_stats['review_count'] - 10) / 100, 0, 1)\n",
        "\n",
        "    # c) Bias detection (0-1)\n",
        "    # Skrajne positive_ratio (blisko 0 lub 1) = podejrzane\n",
        "    bias_score = np.abs(reviewer_stats['positive_ratio'] - 0.5) * 2\n",
        "\n",
        "    # d) Score consistency (0-1)\n",
        "    # Bardzo niskie std = podejrzane (bot-like behavior)\n",
        "    consistency_score = 1 - np.clip(reviewer_stats['score_std'] / 2.5, 0, 1)\n",
        "\n",
        "    # e) Early review pattern (0-1)\n",
        "    # Częste bardzo wczesne recenzje = podejrzane\n",
        "    early_score = np.clip((7 - reviewer_stats['avg_days_since_release']) / 7, 0, 1)\n",
        "    early_score = np.maximum(early_score, 0)\n",
        "\n",
        "    # 3. Weighted composite (wagi do dostrojenia)\n",
        "    weights = {\n",
        "        'burst': 0.30,       # Najważniejszy sygnał\n",
        "        'velocity': 0.15,\n",
        "        'bias': 0.25,        # Drugi najważniejszy\n",
        "        'consistency': 0.20,\n",
        "        'early': 0.10\n",
        "    }\n",
        "\n",
        "    suspicious_raw = (\n",
        "        weights['burst'] * burst_score +\n",
        "        weights['velocity'] * velocity_score +\n",
        "        weights['bias'] * bias_score +\n",
        "        weights['consistency'] * consistency_score +\n",
        "        weights['early'] * early_score\n",
        "    )\n",
        "\n",
        "    # 4. Skaluj do 1-10\n",
        "    reviewer_stats['suspicious_score'] = 1 + (suspicious_raw * 9)\n",
        "\n",
        "    # 5. Merge z powrotem do df\n",
        "    df = df.merge(\n",
        "        reviewer_stats[['critic_name', 'suspicious_score']],\n",
        "        on='critic_name',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    return df, reviewer_stats  # ZWRACAMY OBA\n",
        "\n",
        "\n",
        "# Użycie - ODBIERAMY OBA\n",
        "df, reviewer_stats = calculate_reviewer_suspicious_score(df)\n"
      ],
      "metadata": {
        "id": "hA2svjgAkp4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# WIZUALIZACJA: Histogram + Top 20\n",
        "# ========================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SUSPICIOUS SCORE ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Statystyki\n",
        "print(f\"\\n📊 Suspicious Score Statistics:\")\n",
        "print(reviewer_stats['suspicious_score'].describe())\n",
        "print(f\"\\nTotal reviewers: {len(reviewer_stats)}\")\n",
        "print(f\"Highly suspicious (score > 7): {(reviewer_stats['suspicious_score'] > 7).sum()} ({(reviewer_stats['suspicious_score'] > 7).sum()/len(reviewer_stats)*100:.1f}%)\")\n",
        "\n",
        "# Wizualizacja\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 1. Histogram\n",
        "axes[0].hist(reviewer_stats['suspicious_score'], bins=30, edgecolor='black',\n",
        "            color='#FF6B6B', alpha=0.7)\n",
        "axes[0].axvline(reviewer_stats['suspicious_score'].median(),\n",
        "               color='blue', linestyle='--', linewidth=2,\n",
        "               label=f\"Median: {reviewer_stats['suspicious_score'].median():.2f}\")\n",
        "axes[0].axvline(reviewer_stats['suspicious_score'].mean(),\n",
        "               color='green', linestyle='--', linewidth=2,\n",
        "               label=f\"Mean: {reviewer_stats['suspicious_score'].mean():.2f}\")\n",
        "axes[0].set_xlabel('Suspicious Score (1-10)', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Reviewers', fontsize=12)\n",
        "axes[0].set_title('Distribution of Reviewer Suspicious Scores',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "axes[0].set_xlim(0, 11)\n",
        "\n",
        "# 2. Top 20 bar chart\n",
        "top_20_suspicious = reviewer_stats.nlargest(20, 'suspicious_score')\n",
        "axes[1].barh(range(20), top_20_suspicious['suspicious_score'].values,\n",
        "            color='#FF3B3F', edgecolor='black')\n",
        "axes[1].set_yticks(range(20))\n",
        "axes[1].set_yticklabels(top_20_suspicious['critic_name'].values, fontsize=9)\n",
        "axes[1].set_xlabel('Suspicious Score', fontsize=12)\n",
        "axes[1].set_title('Top 20 Most Suspicious Critics',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "axes[1].invert_yaxis()\n",
        "axes[1].grid(axis='x', alpha=0.3)\n",
        "axes[1].axvline(7, color='orange', linestyle='--', linewidth=1.5,\n",
        "               alpha=0.7, label='High suspicion threshold')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✅ Suspicious score calculation complete!\")\n"
      ],
      "metadata": {
        "id": "lNRIqk49yzrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ZMIANY DO RECENZENTÓW\n",
        "\n",
        "- usunąć burst 24h, zamienić cechę czasową w modelu nr.3 na burst 7-dniowy - 50 recenzji na tydzień to i tak podejrzanie sporo w realnych warunkach, a daje lepszy pogląd"
      ],
      "metadata": {
        "id": "lEnvF1ARuWo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) próg \"podejrzanego\" recenzenta\n",
        "SUS_TH = 7.0\n",
        "# alternatywnie: SUS_TH = df['suspicious_score'].quantile(0.995)\n",
        "\n",
        "# ROZWIĄZANIE: Obsłuż NA przed konwersją na int\n",
        "df['is_suspicious_reviewer'] = (\n",
        "    (df['suspicious_score'] >= SUS_TH)\n",
        "    .fillna(False)  # Zamień NA na False\n",
        "    .astype(int)\n",
        ")\n",
        "\n",
        "movie_susp = (\n",
        "    df.groupby('movie_title')\n",
        "      .agg(\n",
        "          suspicious_reviews=('is_suspicious_reviewer', 'sum'),\n",
        "          total_reviews=('is_suspicious_reviewer', 'size'),\n",
        "          suspicious_ratio=('is_suspicious_reviewer', 'mean'),\n",
        "          suspicious_reviewers=('critic_name', lambda s: s[df.loc[s.index, 'is_suspicious_reviewer'].eq(1)].nunique())\n",
        "      )\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "MIN_TOTAL = 10  # żeby nie pompować filmów z małą liczbą recenzji\n",
        "movie_susp = movie_susp[movie_susp['total_reviews'] >= MIN_TOTAL].copy()\n",
        "\n",
        "movie_susp = movie_susp.sort_values(\n",
        "    ['suspicious_reviews', 'suspicious_ratio', 'total_reviews'],\n",
        "    ascending=[False, False, False]\n",
        ")\n",
        "\n",
        "movie_susp.head(20)\n"
      ],
      "metadata": {
        "id": "KDZrjb-1L-yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_susp_studio = movie_susp.merge(\n",
        "    movies_df[['movie_title', 'production_company']].drop_duplicates(),\n",
        "    on='movie_title',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "movie_susp_studio.head(20)\n"
      ],
      "metadata": {
        "id": "bOWg9xu9bAHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A) SUMA podejrzanych recenzji per studio\n",
        "studio_rank = (\n",
        "    movie_susp_studio\n",
        "      .groupby('production_company', dropna=False)\n",
        "      .agg(\n",
        "          suspicious_reviews=('suspicious_reviews', 'sum'),\n",
        "          total_reviews=('total_reviews', 'sum'),\n",
        "          suspicious_ratio=('suspicious_ratio', 'mean'),   # średnia z filmów (nie ważona)\n",
        "          movies=('movie_title', 'nunique')\n",
        "      )\n",
        "      .reset_index()\n",
        "      .sort_values(['suspicious_reviews', 'movies'], ascending=[False, False])\n",
        ")\n",
        "\n",
        "studio_rank.head(20)\n",
        "\n",
        "# B) Studia najczęściej występujące w TOP-N filmów\n",
        "TOP_N = 200\n",
        "topN = movie_susp_studio.head(TOP_N)\n",
        "\n",
        "studio_topN = (\n",
        "    topN.groupby('production_company', dropna=False)\n",
        "        .agg(\n",
        "            movies_in_topN=('movie_title', 'nunique'),\n",
        "            suspicious_reviews_in_topN=('suspicious_reviews', 'sum')\n",
        "        )\n",
        "        .reset_index()\n",
        "        .sort_values(['movies_in_topN', 'suspicious_reviews_in_topN'], ascending=[False, False])\n",
        ")\n",
        "\n",
        "studio_topN.head(20)\n"
      ],
      "metadata": {
        "id": "1zbOluzebDJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL 3 //////////\n"
      ],
      "metadata": {
        "id": "BpwEQf-Qk_mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ====================================================================================================\n",
        "# MODEL 3: HYBRID ISOLATION FOREST - FAKE REVIEW DETECTION\n",
        "# ====================================================================================================\n",
        "# Unsupervised anomaly detection using numerical features + BERT text embeddings\n",
        "# Input: DataFrame 'df' z wszystkimi cechami z poprzednich etapów (Model 1, Model 2, EDA)\n",
        "# Output: fraud_probability (0-100%) + is_fake_review (binary)\n",
        "# ====================================================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import umap.umap_ as umap\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"MODEL 3: HYBRID ISOLATION FOREST FOR FAKE REVIEW DETECTION\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# ====================================================================================================\n",
        "# STEP 1: SELECT & PREPARE NUMERICAL FEATURES\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n🔧 STEP 1: Preparing numerical features...\")\n",
        "\n",
        "# Wybierz features (wszystkie są już w DataFrame z poprzednich kroków)\n",
        "numerical_features = [\n",
        "    'reviewer_reviews_last_24h',   # Bot burst detection\n",
        "    'reviewer_burst_ratio_24h',    # Burst concentration\n",
        "    'positive_ratio',               # Bias detection\n",
        "    'score_std',                    # Consistency check\n",
        "    'days_since_release',           # Timing analysis\n",
        "    'lexical_diversity',            # Text quality\n",
        "    'avg_word_length',              # Language complexity\n",
        "    'word_count',                   # Length check\n",
        "    'score_residual',               # Model 2 error\n",
        "    'suspicious_score'              # Composite score\n",
        "]\n",
        "\n",
        "# OPTIONAL: Dodaj więcej cech dla lepszych wyników\n",
        "optional_features = [\n",
        "    'is_reviewer_burst_any',        # Binary burst flag\n",
        "    'days_active',                  # Account age\n",
        "    'unique_movies',                # Spam detection\n",
        "    'review_count',                 # Volume indicator\n",
        "    'sentence_count'                # Quality metric\n",
        "]\n",
        "\n",
        "# Wybierz: podstawowe albo podstawowe + optional\n",
        "use_optional = True  # Zmień na False jeśli chcesz tylko 10 podstawowych\n",
        "\n",
        "if use_optional:\n",
        "    # Sprawdź które optional features istnieją w df\n",
        "    available_optional = [f for f in optional_features if f in df.columns]\n",
        "    all_features = numerical_features + available_optional\n",
        "    print(f\"   Using {len(numerical_features)} base + {len(available_optional)} optional = {len(all_features)} features\")\n",
        "else:\n",
        "    all_features = numerical_features\n",
        "    print(f\"   Using {len(all_features)} base features only\")\n",
        "\n",
        "# Filtruj DataFrame - usuń wiersze z brakującymi kluczowymi cechami\n",
        "print(\"\\n📊 Filtering data...\")\n",
        "print(f\"   Original size: {len(df):,} reviews\")\n",
        "\n",
        "# Musi mieć text_cleaned i przynajmniej podstawowe numerical features\n",
        "required_cols = ['text_cleaned'] + numerical_features\n",
        "df_model3 = df[df[required_cols].notna().all(axis=1)].copy()\n",
        "\n",
        "print(f\"   After filtering: {len(df_model3):,} reviews ({len(df_model3)/len(df)*100:.1f}%)\")\n",
        "print(f\"   Removed: {len(df) - len(df_model3):,} reviews with missing data\")\n",
        "\n",
        "# Handle missing values w optional features (jeśli używane)\n",
        "if use_optional:\n",
        "    for col in available_optional:\n",
        "        if df_model3[col].isna().sum() > 0:\n",
        "            # Fill with median for numerical, 0 for binary\n",
        "            if df_model3[col].dtype == bool or df_model3[col].nunique() == 2:\n",
        "                df_model3[col] = df_model3[col].fillna(0).astype(int)\n",
        "            else:\n",
        "                df_model3[col] = df_model3[col].fillna(df_model3[col].median())\n",
        "\n",
        "# Extract numerical features\n",
        "X_numerical = df_model3[all_features].values\n",
        "\n",
        "# Standardize (ważne dla IF!)\n",
        "scaler = StandardScaler()\n",
        "X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
        "\n",
        "print(f\"✅ Numerical features prepared: {X_numerical_scaled.shape}\")"
      ],
      "metadata": {
        "id": "2WbFCeP4k_wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================================================\n",
        "# STEP 2: GENERATE TEXT EMBEDDINGS (BERT)\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n🤖 STEP 2: Generating BERT text embeddings...\")\n",
        "print(\"   Model: sentence-transformers/all-MiniLM-L6-v2 (384 dims)\")\n",
        "print(\"   This may take a few minutes for large datasets...\")\n",
        "\n",
        "# Initialize model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Encode text (batch processing for speed)\n",
        "text_list = df_model3['text_cleaned'].tolist()\n",
        "\n",
        "# Progress bar\n",
        "text_embeddings = model.encode(\n",
        "    text_list,\n",
        "    batch_size=64,              # Zwiększ do 128 jeśli masz GPU\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True,\n",
        "    normalize_embeddings=True   # L2 normalization (helps UMAP)\n",
        ")\n",
        "\n",
        "print(f\"✅ Text embeddings generated: {text_embeddings.shape}\")\n",
        "\n",
        "# ====================================================================================================\n",
        "# STEP 3: DIMENSIONALITY REDUCTION (UMAP)\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n📉 STEP 3: Reducing text embeddings with UMAP...\")\n",
        "print(\"   384 dims → 10 dims (preserving semantic structure)\")\n",
        "\n",
        "reducer = umap.UMAP(\n",
        "    n_components=10,            # Target dimensions\n",
        "    n_neighbors=15,             # Local structure preservation\n",
        "    min_dist=0.1,               # Minimum distance in low-dim space\n",
        "    metric='cosine',            # Best for text embeddings\n",
        "    random_state=42,\n",
        "    n_jobs=-1                   # Use all CPU cores\n",
        ")\n",
        "\n",
        "text_embeddings_reduced = reducer.fit_transform(text_embeddings)\n",
        "\n",
        "print(f\"✅ UMAP reduction complete: {text_embeddings_reduced.shape}\")\n",
        "\n",
        "# ====================================================================================================\n",
        "# STEP 4: COMBINE FEATURES\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n🔗 STEP 4: Combining numerical + text features...\")\n",
        "\n",
        "# Concatenate: [numerical (10-15 dims) + text (10 dims)]\n",
        "combined_features = np.hstack([\n",
        "    X_numerical_scaled,         # Scaled numerical\n",
        "    text_embeddings_reduced     # UMAP-reduced text\n",
        "])\n",
        "\n",
        "print(f\"✅ Combined feature matrix: {combined_features.shape}\")\n",
        "print(f\"   Composition: {X_numerical_scaled.shape[1]} numerical + {text_embeddings_reduced.shape[1]} text\")"
      ],
      "metadata": {
        "id": "vyGTe4qVz7Yf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================================================\n",
        "# STEP 5: ISOLATION FOREST TRAINING\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n🌲 STEP 5: Training Isolation Forest...\")\n",
        "\n",
        "# Hyperparameters\n",
        "contamination = 0.05            # Assume 5% are fake (adjust based on domain knowledge)\n",
        "n_estimators = 200              # Number of trees (more = more stable)\n",
        "max_samples = 256               # Subsample size per tree\n",
        "\n",
        "print(f\"   Contamination: {contamination*100:.1f}% (top {contamination*100}% flagged as anomalies)\")\n",
        "print(f\"   Estimators: {n_estimators}\")\n",
        "print(f\"   Max samples: {max_samples}\")\n",
        "\n",
        "iso_forest = IsolationForest(\n",
        "    contamination=contamination,\n",
        "    n_estimators=n_estimators,\n",
        "    max_samples=max_samples,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,                  # Parallel processing\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "print(\"   Training...\")\n",
        "iso_forest.fit(combined_features)\n",
        "\n",
        "# Predict\n",
        "anomaly_scores = iso_forest.decision_function(combined_features)  # Continuous score\n",
        "predictions = iso_forest.predict(combined_features)               # -1 = anomaly, 1 = normal\n",
        "\n",
        "print(\"✅ Training complete!\")\n",
        "\n",
        "# ====================================================================================================\n",
        "# STEP 6: PROCESS RESULTS\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n📊 STEP 6: Processing results...\")\n",
        "\n",
        "# Add to DataFrame\n",
        "df_model3['anomaly_score'] = anomaly_scores\n",
        "df_model3['is_fake_review'] = (predictions == -1).astype(int)\n",
        "\n",
        "# Fraud probability (0-100%, normalized)\n",
        "# Niższy anomaly_score = wyższe fraud probability\n",
        "min_score = anomaly_scores.min()\n",
        "max_score = anomaly_scores.max()\n",
        "df_model3['fraud_probability'] = 100 * (1 - (anomaly_scores - min_score) / (max_score - min_score))\n",
        "\n",
        "# Statistics\n",
        "fake_count = df_model3['is_fake_review'].sum()\n",
        "fake_pct = fake_count / len(df_model3) * 100\n",
        "\n",
        "print(f\"\\n✅ Detection complete!\")\n",
        "print(f\"   Total reviews analyzed: {len(df_model3):,}\")\n",
        "print(f\"   Flagged as fake: {fake_count:,} ({fake_pct:.2f}%)\")\n",
        "print(f\"   Legit reviews: {len(df_model3) - fake_count:,} ({100-fake_pct:.2f}%)\")"
      ],
      "metadata": {
        "id": "qk8ons100AqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================================================\n",
        "# STEP 7: ANALYSIS & VALIDATION\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n🔍 STEP 7: Analyzing results...\")\n",
        "\n",
        "# Top suspicious reviews\n",
        "top_n = 100\n",
        "top_suspicious = df_model3.nlargest(top_n, 'fraud_probability')\n",
        "\n",
        "print(f\"\\nTop {top_n} most suspicious reviews:\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "# Wyświetl top 10\n",
        "display_cols = ['fraud_probability', 'reviewer_reviews_last_24h', 'positive_ratio',\n",
        "                'days_since_release', 'lexical_diversity', 'text_cleaned']\n",
        "available_display = [col for col in display_cols if col in top_suspicious.columns]\n",
        "\n",
        "for i, (idx, row) in enumerate(top_suspicious.head(10).iterrows(), 1):\n",
        "    print(f\"\\n{i}. Fraud probability: {row['fraud_probability']:.1f}%\")\n",
        "    print(f\"   Text: {row['text_cleaned'][:100]}...\")\n",
        "    print(f\"   Burst 24h: {row.get('reviewer_reviews_last_24h', 'N/A')}, \"\n",
        "          f\"Positive ratio: {row.get('positive_ratio', 'N/A'):.2f}, \"\n",
        "          f\"Days since release: {row.get('days_since_release', 'N/A')}\")\n",
        "\n",
        "# Compare fake vs legit\n",
        "print(\"\\n\" + \"=\" * 100)\n",
        "print(\"FEATURE COMPARISON: FAKE vs LEGIT\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "fake_df = df_model3[df_model3['is_fake_review'] == 1]\n",
        "legit_df = df_model3[df_model3['is_fake_review'] == 0]\n",
        "\n",
        "comparison_features = ['reviewer_reviews_last_24h', 'positive_ratio', 'lexical_diversity',\n",
        "                       'days_since_release', 'score_std']\n",
        "available_comparison = [f for f in comparison_features if f in df_model3.columns]\n",
        "\n",
        "print(f\"\\n{'Feature':<30} {'Fake (mean)':<15} {'Legit (mean)':<15} {'Difference':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for feat in available_comparison:\n",
        "    fake_mean = fake_df[feat].mean()\n",
        "    legit_mean = legit_df[feat].mean()\n",
        "    diff = fake_mean - legit_mean\n",
        "    print(f\"{feat:<30} {fake_mean:<15.2f} {legit_mean:<15.2f} {diff:<15.2f}\")\n",
        "\n",
        "# ====================================================================================================\n",
        "# STEP 8: VISUALIZATIONS\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n📈 STEP 8: Generating visualizations...\")\n",
        "\n",
        "fig = plt.figure(figsize=(18, 12))\n",
        "\n",
        "# 1. Fraud probability distribution\n",
        "ax1 = plt.subplot(3, 3, 1)\n",
        "ax1.hist(df_model3['fraud_probability'], bins=50, edgecolor='black', color='#FF6B6B', alpha=0.7)\n",
        "threshold_95 = df_model3['fraud_probability'].quantile(0.95)\n",
        "ax1.axvline(threshold_95, color='red', linestyle='--', linewidth=2,\n",
        "            label=f'95th percentile: {threshold_95:.1f}%')\n",
        "ax1.set_xlabel('Fraud Probability (%)', fontsize=10)\n",
        "ax1.set_ylabel('Count', fontsize=10)\n",
        "ax1.set_title('Distribution of Fraud Scores', fontsize=12, fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# 2. Anomaly score distribution\n",
        "ax2 = plt.subplot(3, 3, 2)\n",
        "ax2.hist(df_model3['anomaly_score'], bins=50, edgecolor='black', color='#4ECDC4', alpha=0.7)\n",
        "ax2.axvline(0, color='red', linestyle='--', linewidth=2, label='Decision boundary')\n",
        "ax2.set_xlabel('Anomaly Score', fontsize=10)\n",
        "ax2.set_ylabel('Count', fontsize=10)\n",
        "ax2.set_title('Isolation Forest Anomaly Scores', fontsize=12, fontweight='bold')\n",
        "ax2.legend()\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "# 3. Fake vs Legit count\n",
        "ax3 = plt.subplot(3, 3, 3)\n",
        "counts = df_model3['is_fake_review'].value_counts()\n",
        "colors = ['#4ECDC4', '#FF6B6B']\n",
        "ax3.bar(['Legit', 'Fake'], [counts.get(0, 0), counts.get(1, 0)], color=colors, edgecolor='black')\n",
        "ax3.set_ylabel('Count', fontsize=10)\n",
        "ax3.set_title('Fake vs Legit Reviews', fontsize=12, fontweight='bold')\n",
        "for i, v in enumerate([counts.get(0, 0), counts.get(1, 0)]):\n",
        "    ax3.text(i, v + 100, f'{v:,}\\n({v/len(df_model3)*100:.1f}%)', ha='center', fontweight='bold')\n",
        "ax3.grid(alpha=0.3, axis='y')\n",
        "\n",
        "# 4. Feature comparison: reviewer_reviews_last_24h\n",
        "if 'reviewer_reviews_last_24h' in df_model3.columns:\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    ax4.boxplot([legit_df['reviewer_reviews_last_24h'], fake_df['reviewer_reviews_last_24h']],\n",
        "                labels=['Legit', 'Fake'], patch_artist=True)\n",
        "    ax4.set_ylabel('Reviews in 24h', fontsize=10)\n",
        "    ax4.set_title('Burst Activity: Fake vs Legit', fontsize=12, fontweight='bold')\n",
        "    ax4.grid(alpha=0.3, axis='y')\n",
        "\n",
        "# 5. Feature comparison: positive_ratio\n",
        "if 'positive_ratio' in df_model3.columns:\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    ax5.hist(legit_df['positive_ratio'], bins=30, alpha=0.6, label='Legit', color='#4ECDC4', edgecolor='black')\n",
        "    ax5.hist(fake_df['positive_ratio'], bins=30, alpha=0.6, label='Fake', color='#FF6B6B', edgecolor='black')\n",
        "    ax5.axvline(0.95, color='red', linestyle='--', alpha=0.5)\n",
        "    ax5.axvline(0.05, color='red', linestyle='--', alpha=0.5)\n",
        "    ax5.set_xlabel('Positive Ratio', fontsize=10)\n",
        "    ax5.set_ylabel('Count', fontsize=10)\n",
        "    ax5.set_title('Bias Detection', fontsize=12, fontweight='bold')\n",
        "    ax5.legend()\n",
        "    ax5.grid(alpha=0.3)\n",
        "\n",
        "# 6. Feature comparison: lexical_diversity\n",
        "if 'lexical_diversity' in df_model3.columns:\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    ax6.hist(legit_df['lexical_diversity'], bins=30, alpha=0.6, label='Legit', color='#4ECDC4', edgecolor='black')\n",
        "    ax6.hist(fake_df['lexical_diversity'], bins=30, alpha=0.6, label='Fake', color='#FF6B6B', edgecolor='black')\n",
        "    ax6.set_xlabel('Lexical Diversity', fontsize=10)\n",
        "    ax6.set_ylabel('Count', fontsize=10)\n",
        "    ax6.set_title('Text Quality', fontsize=12, fontweight='bold')\n",
        "    ax6.legend()\n",
        "    ax6.grid(alpha=0.3)\n",
        "\n",
        "# 7. Scatter: burst vs positive_ratio\n",
        "if 'reviewer_reviews_last_24h' in df_model3.columns and 'positive_ratio' in df_model3.columns:\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    ax7.scatter(legit_df['reviewer_reviews_last_24h'], legit_df['positive_ratio'],\n",
        "                alpha=0.3, s=10, color='#4ECDC4', label='Legit')\n",
        "    ax7.scatter(fake_df['reviewer_reviews_last_24h'], fake_df['positive_ratio'],\n",
        "                alpha=0.5, s=20, color='#FF6B6B', marker='x', label='Fake')\n",
        "    ax7.set_xlabel('Reviews in 24h', fontsize=10)\n",
        "    ax7.set_ylabel('Positive Ratio', fontsize=10)\n",
        "    ax7.set_title('Multi-dimensional Anomalies', fontsize=12, fontweight='bold')\n",
        "    ax7.legend()\n",
        "    ax7.grid(alpha=0.3)\n",
        "\n",
        "# 8. Feature importance (correlation proxy)\n",
        "ax8 = plt.subplot(3, 3, 8)\n",
        "feature_names = all_features + [f'text_emb_{i}' for i in range(10)]\n",
        "feature_matrix = pd.DataFrame(combined_features, columns=feature_names)\n",
        "feature_matrix['anomaly_score'] = anomaly_scores\n",
        "correlations = feature_matrix.corr()['anomaly_score'].drop('anomaly_score').abs().sort_values(ascending=False)\n",
        "top_corr = correlations.head(15)\n",
        "ax8.barh(range(len(top_corr)), top_corr.values, color='#45B7D1', edgecolor='black')\n",
        "ax8.set_yticks(range(len(top_corr)))\n",
        "ax8.set_yticklabels(top_corr.index, fontsize=8)\n",
        "ax8.set_xlabel('Abs Correlation', fontsize=10)\n",
        "ax8.set_title('Feature Importance (Proxy)', fontsize=12, fontweight='bold')\n",
        "ax8.invert_yaxis()\n",
        "ax8.grid(alpha=0.3, axis='x')\n",
        "\n",
        "# 9. Fraud probability by days_since_release\n",
        "if 'days_since_release' in df_model3.columns:\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    bins = [-np.inf, 7, 30, 90, np.inf]\n",
        "    labels = ['<7d', '7-30d', '30-90d', '>90d']\n",
        "    df_model3['release_bin'] = pd.cut(df_model3['days_since_release'], bins=bins, labels=labels)\n",
        "    fraud_by_timing = df_model3.groupby('release_bin')['fraud_probability'].mean()\n",
        "    ax9.bar(range(len(fraud_by_timing)), fraud_by_timing.values, color='#FF6B6B', edgecolor='black')\n",
        "    ax9.set_xticks(range(len(fraud_by_timing)))\n",
        "    ax9.set_xticklabels(fraud_by_timing.index)\n",
        "    ax9.set_ylabel('Avg Fraud Probability (%)', fontsize=10)\n",
        "    ax9.set_title('Fraud Risk by Review Timing', fontsize=12, fontweight='bold')\n",
        "    ax9.grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('model3_fake_review_detection_analysis.png', dpi=150, bbox_inches='tight')\n",
        "print(\"   ✅ Saved: model3_fake_review_detection_analysis.png\")\n",
        "plt.show()\n",
        "\n",
        "# ====================================================================================================\n",
        "# STEP 9: SAVE RESULTS\n",
        "# ====================================================================================================\n",
        "\n",
        "print(\"\\n💾 STEP 9: Saving results...\")\n",
        "\n",
        "# Select columns to save\n",
        "output_cols = [\n",
        "    'fraud_probability', 'is_fake_review', 'anomaly_score',\n",
        "    'text_cleaned', 'review_content',\n",
        "    'reviewer_reviews_last_24h', 'positive_ratio', 'days_since_release',\n",
        "    'lexical_diversity', 'score_std', 'suspicious_score',\n",
        "    'movie_title', 'critic_name', 'review_date'\n",
        "]\n",
        "\n",
        "# Only keep columns that exist\n",
        "output_cols_available = [col for col in output_cols if col in df_model3.columns]\n",
        "\n",
        "# Save all results\n",
        "df_model3[output_cols_available].to_csv('model3_all_results.csv', index=False, encoding='utf-8')\n",
        "print(\"   ✅ Saved: model3_all_results.csv\")\n",
        "\n",
        "# Save top suspicious for manual inspection\n",
        "top_suspicious[output_cols_available].to_csv('model3_top_suspicious.csv', index=False, encoding='utf-8')\n",
        "print(f\"   ✅ Saved: model3_top_suspicious.csv (top {top_n})\")\n",
        "\n",
        "# Save summary statistics\n",
        "with open('model3_summary.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(\"=\" * 100 + \"\\n\")\n",
        "    f.write(\"MODEL 3: HYBRID ISOLATION FOREST - SUMMARY\\n\")\n",
        "    f.write(\"=\" * 100 + \"\\n\\n\")\n",
        "    f.write(f\"Total reviews analyzed: {len(df_model3):,}\\n\")\n",
        "    f.write(f\"Flagged as fake: {fake_count:,} ({fake_pct:.2f}%)\\n\")\n",
        "    f.write(f\"Contamination threshold: {contamination*100}%\\n\\n\")\n",
        "    f.write(\"Feature configuration:\\n\")\n",
        "    f.write(f\"  Numerical features: {len(all_features)}\\n\")\n",
        "    f.write(f\"  Text embedding dims: 10 (UMAP-reduced from 384)\\n\")\n",
        "    f.write(f\"  Total dimensions: {combined_features.shape[1]}\\n\\n\")\n",
        "    f.write(\"Top features by correlation with anomaly score:\\n\")\n",
        "    for i, (feat, corr) in enumerate(correlations.head(10).items(), 1):\n",
        "        f.write(f\"  {i}. {feat}: {corr:.3f}\\n\")\n",
        "\n",
        "print(\"   ✅ Saved: model3_summary.txt\")\n",
        "\n",
        "print(\"=\" * 100)\n",
        "print(\"🎉 HYBRID ISOLATION FOREST MODEL 3 FINISHED SUCCESSFULLY!\")\n",
        "print(\"=\" * 100)"
      ],
      "metadata": {
        "id": "XXDuXNH30FUY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}